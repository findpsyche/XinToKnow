# ç³»ç»Ÿå·¥å…·é“¾ä¸åº•å±‚æŠ€èƒ½ - AIèŠ¯ç‰‡å¼€å‘å¿…å¤‡

## äºŒã€ç³»ç»Ÿä¸å·¥å…·é“¾ (System & Toolchain)

> è¿™æ˜¯ä½ ä¸"è‡ªç ”èŠ¯ç‰‡"æ‰“äº¤é“çš„å·¥å…·ï¼Œéœ€è¦å…·å¤‡åº•å±‚ç³»ç»Ÿæ„è¯†

---

### 2.1 Linuxæ“ä½œç³»ç»Ÿä¸Shell â­â­â­â­â­

#### ğŸ“š ä¸ºä»€ä¹ˆLinuxå¦‚æ­¤é‡è¦?

```
AIèŠ¯ç‰‡å¼€å‘ä¸­çš„Linuxåº”ç”¨åœºæ™¯:
â”œâ”€â”€ å¼€å‘ç¯å¢ƒ: ç»å¤§å¤šæ•°AIæ¡†æ¶è¿è¡Œåœ¨Linux
â”œâ”€â”€ æœåŠ¡å™¨éƒ¨ç½²: ç”Ÿäº§ç¯å¢ƒ99%æ˜¯Linux
â”œâ”€â”€ èŠ¯ç‰‡é©±åŠ¨: åº•å±‚é©±åŠ¨é€šå¸¸åªæ”¯æŒLinux
â”œâ”€â”€ Dockerå®¹å™¨: åŸºäºLinuxå†…æ ¸
â””â”€â”€ æ€§èƒ½è°ƒä¼˜: Linuxæä¾›ä¸°å¯Œçš„æ€§èƒ½åˆ†æå·¥å…·
```

---

#### ğŸ› ï¸ æ ¸å¿ƒå‘½ä»¤æ·±åº¦ç†è§£

**æ–‡ä»¶æƒé™ç®¡ç† (chmod)**

```bash
# æƒé™è¡¨ç¤ºæ³•

# å­—æ¯è¡¨ç¤ºæ³•:
# r (read=4), w (write=2), x (execute=1)
# user, group, others

# æŸ¥çœ‹æƒé™
ls -l script.sh
# -rwxr-xr-x  1 user group  1024 Jan  7 10:00 script.sh
#  |||\_/\_/
#  |||  |  \_ othersæƒé™: r-x (è¯»+æ‰§è¡Œ) = 5
#  |||  \____ groupæƒé™: r-x = 5  
#  |||\_______ useræƒé™: rwx (è¯»+å†™+æ‰§è¡Œ) = 7
#  ||\________ æ–‡ä»¶ç±»å‹: - (æ™®é€šæ–‡ä»¶), d (ç›®å½•), l (é“¾æ¥)
#  |\_________ åç»­å­—ç¬¦ä¸ºæƒé™ä½

# ä¿®æ”¹æƒé™
chmod 755 script.sh  # ç­‰ä»·äº chmod u=rwx,g=rx,o=rx script.sh
chmod +x script.sh   # æ·»åŠ æ‰§è¡Œæƒé™(æ‰€æœ‰äºº)
chmod u+x script.sh  # åªç»™useræ·»åŠ æ‰§è¡Œæƒé™

# é€’å½’ä¿®æ”¹ç›®å½•
chmod -R 755 /path/to/dir

# å®é™…åœºæ™¯:
# 1. èŠ¯ç‰‡SDKçš„.soåº“éœ€è¦æ‰§è¡Œæƒé™
chmod +x /opt/chip-sdk/lib/*.so

# 2. éƒ¨ç½²è„šæœ¬éœ€è¦å¯æ‰§è¡Œ
chmod +x deploy.sh

# 3. é…ç½®æ–‡ä»¶åªè¯»
chmod 644 config.yaml  # rw-r--r--
```

**è¿›ç¨‹ç®¡ç† (ps, kill, htop)**

```bash
# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep python
# a: æ‰€æœ‰ç”¨æˆ·è¿›ç¨‹
# u: æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
# x: åŒ…æ‹¬æ— ç»ˆç«¯è¿›ç¨‹

# è¾“å‡ºè§£é‡Š:
# USER  PID  %CPU %MEM    VSZ   RSS TTY   STAT START   TIME COMMAND
# user  1234  95.2 15.3 8123456 2048000 ?  Sl   10:00  2:15 python inference_server.py
#       ^^^^ ^^^^ ^^^^
#       è¿›ç¨‹ID CPU% å†…å­˜%

# å¸¸ç”¨ç»„åˆ:
# æ‰¾åˆ°å ç”¨GPUçš„è¿›ç¨‹
nvidia-smi  # æŸ¥çœ‹GPUè¿›ç¨‹åˆ—è¡¨

# æ‰¾åˆ°å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -i :8000  # æŸ¥çœ‹8000ç«¯å£è¢«è°å ç”¨
netstat -tunlp | grep 8000

# æ€æ­»è¿›ç¨‹
kill 1234        # æ¸©å’Œç»ˆæ­¢ (SIGTERM)
kill -9 1234     # å¼ºåˆ¶ç»ˆæ­¢ (SIGKILL)
killall python   # æ€æ­»æ‰€æœ‰pythonè¿›ç¨‹ (å±é™©!)

# åå°è¿è¡Œ
nohup python server.py > server.log 2>&1 &
# nohup: å¿½ç•¥æŒ‚æ–­ä¿¡å·
# > server.log: é‡å®šå‘æ ‡å‡†è¾“å‡º
# 2>&1: æ ‡å‡†é”™è¯¯ä¹Ÿé‡å®šå‘åˆ°server.log
# &: åå°è¿è¡Œ

# æ›´å¥½çš„æ–¹å¼: tmux/screen
tmux new -s inference  # åˆ›å»ºä¼šè¯
python server.py
# Ctrl+B, D åˆ†ç¦»ä¼šè¯
tmux attach -t inference  # é‡æ–°è¿æ¥

# htop: äº¤äº’å¼è¿›ç¨‹æŸ¥çœ‹å™¨
htop
# å¿«æ·é”®:
# F3: æœç´¢è¿›ç¨‹
# F4: è¿‡æ»¤
# F9: killè¿›ç¨‹
# F6: æ’åº(CPU/MEM)
```

**èµ„æºç›‘æ§**

```bash
# CPUç›‘æ§
top
# å…³é”®æŒ‡æ ‡:
# %Cpu(s): 15.3 us (ç”¨æˆ·ç©ºé—´), 2.1 sy (å†…æ ¸ç©ºé—´), 82.6 id (ç©ºé—²)
# load average: 0.52, 0.58, 0.59 (1åˆ†é’Ÿ, 5åˆ†é’Ÿ, 15åˆ†é’Ÿ)
#   è§£é‡Š: < CPUæ ¸å¿ƒæ•° æ­£å¸¸, > 2Ã—æ ¸å¿ƒæ•° è´Ÿè½½é«˜

# å†…å­˜ç›‘æ§
free -h
#               total    used    free  shared  buff/cache  available
# Mem:           31Gi    8.2Gi   18Gi    1.2Gi        4.8Gi       21Gi
# Swap:         8.0Gi      0B   8.0Gi

# GPUç›‘æ§ (NVIDIA)
nvidia-smi
# å…³é”®ä¿¡æ¯:
# - GPUåˆ©ç”¨ç‡
# - æ˜¾å­˜å ç”¨
# - æ¸©åº¦
# - è¿›ç¨‹åˆ—è¡¨

# æŒç»­ç›‘æ§
watch -n 1 nvidia-smi  # æ¯ç§’åˆ·æ–°

# è‡ªç ”èŠ¯ç‰‡ç›‘æ§ (å‡è®¾å‘½ä»¤)
# å…¥èŒåå­¦ä¹ å…¬å¸æä¾›çš„ç›‘æ§å·¥å…·
# ç±»ä¼¼: xiwang-smi æˆ– chip-monitor

# ç£ç›˜ç›‘æ§
df -h  # ç£ç›˜ç©ºé—´
du -sh /path/to/dir  # ç›®å½•å¤§å°
du -h --max-depth=1 | sort -hr  # æ‰¾å‡ºå¤§æ–‡ä»¶

# å®æ—¶IOç›‘æ§
iotop  # ç±»ä¼¼htop,ä½†ç›‘æ§ç£ç›˜IO

# ç½‘ç»œç›‘æ§
iftop  # å®æ—¶ç½‘ç»œæµé‡
nethogs  # æŒ‰è¿›ç¨‹æŸ¥çœ‹ç½‘ç»œä½¿ç”¨
```

**æ—¥å¿—åˆ†ææŠ€å·§**

```bash
# æŸ¥çœ‹æ—¥å¿—
tail -f server.log  # å®æ—¶æŸ¥çœ‹(follow)
tail -n 100 server.log  # æœ€å100è¡Œ

# æœç´¢é”™è¯¯
grep -i "error" server.log  # ä¸åŒºåˆ†å¤§å°å†™
grep -C 5 "CUDA out of memory" server.log  # å‰å5è¡Œä¸Šä¸‹æ–‡

# ç»Ÿè®¡é”™è¯¯æ•°é‡
grep -c "error" server.log

# å¤šæ–‡ä»¶æœç´¢
grep -r "exception" /var/log/  # é€’å½’æœç´¢

# æ—¥å¿—å»é‡
sort server.log | uniq -c | sort -nr  # ç»Ÿè®¡é‡å¤è¡Œå¹¶æ’åº

# æ—¥å¿—åˆ‡åˆ† (ç”Ÿäº§ç¯å¢ƒå¿…å¤‡)
# ä½¿ç”¨logrotateæˆ–æ‰‹åŠ¨åˆ‡åˆ†
split -l 10000 huge.log small_log_  # æ¯10000è¡Œä¸€ä¸ªæ–‡ä»¶

# å®ç”¨ç»„åˆ: æŸ¥æ‰¾æœ€è¿‘çš„é”™è¯¯
tail -n 1000 server.log | grep "ERROR"
```

---

#### ğŸ”§ ç¯å¢ƒç®¡ç† (Conda/venv)

**Condaæ·±åº¦ä½¿ç”¨**

```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n chip-dev python=3.10
conda activate chip-dev

# æŸ¥çœ‹ç¯å¢ƒ
conda env list
# chip-dev  *  /home/user/miniconda3/envs/chip-dev

# å¯¼å‡ºç¯å¢ƒ
conda env export > environment.yml

# environment.ymlç¤ºä¾‹:
name: chip-dev
channels:
  - pytorch
  - nvidia
  - conda-forge
dependencies:
  - python=3.10
  - pytorch=2.1.0
  - cudatoolkit=11.8
  - pip:
    - transformers==4.35.0
    - fastapi==0.104.1

# ä»ymlåˆ›å»ºç¯å¢ƒ
conda env create -f environment.yml

# å…‹éš†ç¯å¢ƒ
conda create --name chip-dev-backup --clone chip-dev

# æ¸…ç†ç¼“å­˜ (condaä¼šå ç”¨å¤§é‡ç©ºé—´)
conda clean --all

# æ›´æ–°åŒ…
conda update transformers
conda update --all  # æ›´æ–°æ‰€æœ‰åŒ…(æ…ç”¨)

# pipä¸condaæ··ç”¨æŠ€å·§
# ä¼˜å…ˆç”¨condaè£…,condaæ²¡æœ‰å†ç”¨pip
conda install numpy
pip install some-rare-package  # condaä¸­æ²¡æœ‰

# æŸ¥çœ‹ç¯å¢ƒä¸­çš„pipåŒ…
pip list | grep transformers
```

**è™šæ‹Ÿç¯å¢ƒæœ€ä½³å®è·µ**

```bash
# é¡¹ç›®ç»“æ„
chip-project/
â”œâ”€â”€ envs/
â”‚   â””â”€â”€ requirements.txt  # pipä¾èµ–
â”œâ”€â”€ conda-env.yml          # condaç¯å¢ƒ
â”œâ”€â”€ src/
â””â”€â”€ README.md

# requirements.txt å†™æ³•
# å›ºå®šç‰ˆæœ¬ (ç”Ÿäº§ç¯å¢ƒ)
torch==2.1.0
transformers==4.35.0

# å…¼å®¹ç‰ˆæœ¬ (å¼€å‘ç¯å¢ƒ)
torch>=2.0.0,<3.0.0
transformers>=4.30.0

# ä»ç¯å¢ƒç”Ÿæˆrequirements.txt
pip freeze > requirements.txt

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ç¦»çº¿å®‰è£… (èŠ¯ç‰‡æœåŠ¡å™¨å¯èƒ½æ²¡ç½‘)
# 1. ä¸‹è½½wheels
pip download -r requirements.txt -d wheels/

# 2. æ‹·è´åˆ°ç›®æ ‡æœºå™¨
scp -r wheels/ user@server:/path/

# 3. ç¦»çº¿å®‰è£…
pip install --no-index --find-links=wheels/ -r requirements.txt
```

---

#### ğŸ“ Shellè„šæœ¬è‡ªåŠ¨åŒ–

**å¸¸ç”¨è„šæœ¬æ¨¡æ¿**

```bash
#!/bin/bash
# deploy_model.sh - æ¨¡å‹éƒ¨ç½²è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º
set -u  # ä½¿ç”¨æœªå®šä¹‰å˜é‡æŠ¥é”™
set -o pipefail  # ç®¡é“ä¸­ä»»ä½•å‘½ä»¤å¤±è´¥åˆ™å¤±è´¥

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# å‚æ•°æ£€æŸ¥
if [ $# -lt 1 ]; then
    log_error "ç”¨æ³•: $0 <model_name>"
    exit 1
fi

MODEL_NAME=$1
MODEL_DIR="/models/${MODEL_NAME}"

# å‡½æ•°å®šä¹‰
check_dependencies() {
    log_info "æ£€æŸ¥ä¾èµ–..."
    
    # æ£€æŸ¥condaç¯å¢ƒ
    if ! conda env list | grep -q "chip-dev"; then
        log_error "ç¯å¢ƒchip-devä¸å­˜åœ¨"
        exit 1
    fi
    
    # æ£€æŸ¥GPU
    if ! nvidia-smi &> /dev/null; then
        log_error "æœªæ£€æµ‹åˆ°GPU"
        exit 1
    fi
    
    log_info "ä¾èµ–æ£€æŸ¥é€šè¿‡"
}

download_model() {
    log_info "ä¸‹è½½æ¨¡å‹: ${MODEL_NAME}"
    
    if [ -d "${MODEL_DIR}" ]; then
        log_info "æ¨¡å‹å·²å­˜åœ¨,è·³è¿‡ä¸‹è½½"
    else
        python -c "from transformers import AutoModel; \
                   AutoModel.from_pretrained('${MODEL_NAME}')"
    fi
}

quantize_model() {
    log_info "é‡åŒ–æ¨¡å‹..."
    
    python scripts/quantize.py \
        --model_name ${MODEL_NAME} \
        --bits 8 \
        --output ${MODEL_DIR}-int8
    
    if [ $? -eq 0 ]; then
        log_info "é‡åŒ–å®Œæˆ"
    else
        log_error "é‡åŒ–å¤±è´¥"
        exit 1
    fi
}

start_server() {
    log_info "å¯åŠ¨æ¨ç†æœåŠ¡..."
    
    # æ£€æŸ¥ç«¯å£æ˜¯å¦å ç”¨
    if lsof -i:8000 > /dev/null; then
        log_error "ç«¯å£8000å·²è¢«å ç”¨"
        exit 1
    fi
    
    nohup python server.py \
        --model ${MODEL_DIR}-int8 \
        --port 8000 \
        > logs/server.log 2>&1 &
    
    SERVER_PID=$!
    echo $SERVER_PID > server.pid
    
    log_info "æœåŠ¡å·²å¯åŠ¨ (PID: ${SERVER_PID})"
}

# ä¸»æµç¨‹
main() {
    check_dependencies
    download_model
    quantize_model
    start_server
    
    log_info "éƒ¨ç½²å®Œæˆ!"
}

# æ‰§è¡Œ
main
```

**è¿è¡Œè„šæœ¬**

```bash
chmod +x deploy_model.sh
./deploy_model.sh "THUDM/chatglm3-6b"
```

---

### 2.2 å®¹å™¨åŒ–æŠ€æœ¯ (Docker) â­â­â­â­â­

#### ğŸ³ ä¸ºä»€ä¹ˆDockerå¯¹AIèŠ¯ç‰‡å¼€å‘æå…¶é‡è¦?

```
AIèŠ¯ç‰‡ç¯å¢ƒçš„å¤æ‚æ€§:
â”œâ”€â”€ èŠ¯ç‰‡é©±åŠ¨: ç‰¹å®šç‰ˆæœ¬,éš¾ä»¥åœ¨ä¸åŒæœºå™¨å¤ç°
â”œâ”€â”€ CUDA/ROCm: ä¸é©±åŠ¨å¼ºç»‘å®š
â”œâ”€â”€ æ·±åº¦å­¦ä¹ æ¡†æ¶: ä¾èµ–ç‰¹å®šç‰ˆæœ¬çš„ç¼–è¯‘åº“
â”œâ”€â”€ Pythonä¾èµ–: æ•°ç™¾ä¸ªåŒ…,ç‰ˆæœ¬å†²çª
â””â”€â”€ ç³»ç»Ÿåº“: glibc, libstdc++ç­‰

Dockerè§£å†³æ–¹æ¡ˆ:
âœ… ç¯å¢ƒä¸€è‡´æ€§: å¼€å‘ç¯å¢ƒ=æµ‹è¯•ç¯å¢ƒ=ç”Ÿäº§ç¯å¢ƒ
âœ… å¿«é€Ÿéƒ¨ç½²: docker runä¸€é”®å¯åŠ¨
âœ… éš”ç¦»æ€§: ä¸æ±¡æŸ“å®¿ä¸»æœº
âœ… ç‰ˆæœ¬æ§åˆ¶: Dockerfileå³ä»£ç 
```

---

#### ğŸ”§ Dockeræ ¸å¿ƒå‘½ä»¤

**åŸºç¡€æ“ä½œ**

```bash
# æ‹‰å–é•œåƒ
docker pull pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

# æŸ¥çœ‹é•œåƒ
docker images
# REPOSITORY          TAG                              SIZE
# pytorch/pytorch     2.1.0-cuda11.8-cudnn8-runtime   5.84GB

# è¿è¡Œå®¹å™¨
docker run -it \
    --gpus all \              # ä½¿ç”¨æ‰€æœ‰GPU
    -v $(pwd):/workspace \    # æŒ‚è½½å½“å‰ç›®å½•
    -p 8000:8000 \            # ç«¯å£æ˜ å°„
    --name chip-dev \         # å®¹å™¨åç§°
    pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime \
    /bin/bash

# å‚æ•°è§£é‡Š:
# -it: äº¤äº’å¼ç»ˆç«¯
# --gpus all: éœ€è¦nvidia-dockeræ”¯æŒ
# -v: æŒ‚è½½å· (å®¿ä¸»æœºè·¯å¾„:å®¹å™¨è·¯å¾„)
# -p: ç«¯å£æ˜ å°„ (å®¿ä¸»æœºç«¯å£:å®¹å™¨ç«¯å£)
# --name: å®¹å™¨åç§°

# æŸ¥çœ‹è¿è¡Œä¸­çš„å®¹å™¨
docker ps
docker ps -a  # åŒ…æ‹¬åœæ­¢çš„

# è¿›å…¥è¿è¡Œä¸­çš„å®¹å™¨
docker exec -it chip-dev /bin/bash

# åœæ­¢å®¹å™¨
docker stop chip-dev

# å¯åŠ¨å·²åœæ­¢çš„å®¹å™¨
docker start chip-dev

# åˆ é™¤å®¹å™¨
docker rm chip-dev

# åˆ é™¤é•œåƒ
docker rmi pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime
```

**æ•°æ®æŒä¹…åŒ–**

```bash
# æŒ‚è½½æ–¹å¼å¯¹æ¯”

# 1. Bind Mount (ç»‘å®šæŒ‚è½½) - æ¨èå¼€å‘ä½¿ç”¨
docker run -v /host/path:/container/path image

# ä¼˜ç‚¹: 
# - ç›´æ¥è®¿é—®å®¿ä¸»æœºæ–‡ä»¶
# - ä¿®æ”¹ç«‹å³ç”Ÿæ•ˆ
# ç¼ºç‚¹:
# - è·¯å¾„ç¡¬ç¼–ç 
# - æƒé™é—®é¢˜

# 2. Named Volume (å‘½åå·) - æ¨èç”Ÿäº§ä½¿ç”¨
docker volume create models-data
docker run -v models-data:/models image

# ä¼˜ç‚¹:
# - Dockerç®¡ç†,æ— éœ€å…³å¿ƒè·¯å¾„
# - å®¹å™¨é—´å…±äº«
# ç¼ºç‚¹:
# - ä¸æ˜“ç›´æ¥è®¿é—®

# æŸ¥çœ‹å·
docker volume ls
docker volume inspect models-data

# å®é™…åœºæ™¯: æ¨¡å‹æ–‡ä»¶æŒä¹…åŒ–
docker run -it \
    --gpus all \
    -v ~/models:/models:ro \  # åªè¯»æŒ‚è½½,é˜²æ­¢è¯¯åˆ 
    -v ~/code:/workspace \     # ä»£ç ç›®å½•
    -v model-cache:/root/.cache/huggingface \  # ç¼“å­˜ç›®å½•
    pytorch/pytorch:latest \
    /bin/bash
```

---

#### ğŸ“ Dockerfileç¼–å†™

**åŸºç¡€æ¨¡æ¿**

```dockerfile
# Dockerfile - AIèŠ¯ç‰‡æ¨ç†æœåŠ¡

# åŸºç¡€é•œåƒé€‰æ‹©
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

# ç»´æŠ¤è€…ä¿¡æ¯
LABEL maintainer="your-email@example.com"
LABEL description="AI Chip Inference Server"

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    git \
    wget \
    vim \
    && rm -rf /var/lib/apt/lists/*  # æ¸…ç†ç¼“å­˜å‡å°é•œåƒå¤§å°

# å¤åˆ¶requirements.txt
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt \
    && pip install --no-cache-dir \
       transformers==4.35.0 \
       fastapi==0.104.1 \
       uvicorn==0.24.0

# å¤åˆ¶é¡¹ç›®ä»£ç 
COPY src/ ./src/
COPY configs/ ./configs/

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p /models /logs

# æš´éœ²ç«¯å£
EXPOSE 8000

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV MODEL_PATH=/models/chatglm3-6b
ENV LOG_LEVEL=INFO
ENV CUDA_VISIBLE_DEVICES=0

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "src.server:app", "--host", "0.0.0.0", "--port", "8000"]
```

**æ„å»ºé•œåƒ**

```bash
# æ„å»º
docker build -t chip-inference:v1.0 .

# æŒ‡å®šDockerfile
docker build -f Dockerfile.cuda -t chip-inference:cuda .

# ä½¿ç”¨æ„å»ºç¼“å­˜åŠ é€Ÿ
docker build --cache-from chip-inference:v1.0 -t chip-inference:v1.1 .

# å¤šé˜¶æ®µæ„å»º (å‡å°é•œåƒå¤§å°)
```

**å¤šé˜¶æ®µæ„å»ºç¤ºä¾‹**

```dockerfile
# ç¬¬ä¸€é˜¶æ®µ: æ„å»ºç¯å¢ƒ
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel AS builder

WORKDIR /build

# å®‰è£…ç¼–è¯‘ä¾èµ–
RUN pip install wheel

# ç¼–è¯‘è‡ªå®šä¹‰ç®—å­ (å‡è®¾æœ‰C++æ‰©å±•)
COPY setup.py .
COPY csrc/ ./csrc/
RUN python setup.py bdist_wheel

# ç¬¬äºŒé˜¶æ®µ: è¿è¡Œç¯å¢ƒ
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

WORKDIR /app

# åªå¤åˆ¶ç¼–è¯‘å¥½çš„wheel,ä¸åŒ…å«ç¼–è¯‘å·¥å…·
COPY --from=builder /build/dist/*.whl /tmp/
RUN pip install /tmp/*.whl && rm /tmp/*.whl

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY src/ ./src/

CMD ["python", "src/server.py"]

# ä¼˜åŠ¿: æœ€ç»ˆé•œåƒä¸åŒ…å«ç¼–è¯‘å·¥å…·,å¤§å°å‡å°‘50%+
```

---

#### ğŸš€ Docker Composeç¼–æ’

**åœºæ™¯: éƒ¨ç½²æ¨ç†æœåŠ¡ + å‘é‡æ•°æ®åº“**

```yaml
# docker-compose.yml

version: '3.8'

services:
  # æ¨ç†æœåŠ¡
  inference:
    build:
      context: .
      dockerfile: Dockerfile
    image: chip-inference:latest
    container_name: inference-server
    ports:
      - "8000:8000"
    volumes:
      - ~/models:/models:ro
      - ./logs:/logs
    environment:
      - MODEL_PATH=/models/chatglm3-6b-int8
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # å‘é‡æ•°æ®åº“
  milvus:
    image: milvusdb/milvus:v2.3.0
    container_name: milvus-standalone
    ports:
      - "19530:19530"
    volumes:
      - milvus-data:/var/lib/milvus
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000
    depends_on:
      - etcd
      - minio

  # etcd (Milvusä¾èµ–)
  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
    volumes:
      - etcd-data:/etcd

  # MinIO (Milvuså­˜å‚¨)
  minio:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
    volumes:
      - minio-data:/minio_data
    command: minio server /minio_data

volumes:
  milvus-data:
  etcd-data:
  minio-data:

networks:
  default:
    name: chip-ai-network
```

**ä½¿ç”¨Compose**

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f inference

# åœæ­¢æœåŠ¡
docker-compose down

# é‡å¯æŸä¸ªæœåŠ¡
docker-compose restart inference

# æ‰©å±•æœåŠ¡ (å¤šå‰¯æœ¬)
docker-compose up -d --scale inference=3
```

---

#### ğŸ” è°ƒè¯•æŠ€å·§

```bash
# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs chip-dev
docker logs -f chip-dev  # å®æ—¶æŸ¥çœ‹
docker logs --tail 100 chip-dev  # æœ€å100è¡Œ

# æŸ¥çœ‹å®¹å™¨å†…è¿›ç¨‹
docker top chip-dev

# æŸ¥çœ‹å®¹å™¨èµ„æºä½¿ç”¨
docker stats chip-dev

# æŸ¥çœ‹å®¹å™¨è¯¦ç»†ä¿¡æ¯
docker inspect chip-dev

# å¤åˆ¶æ–‡ä»¶
docker cp chip-dev:/app/output.txt ./
docker cp local.txt chip-dev:/app/

# ä¿å­˜ä¿®æ”¹åçš„å®¹å™¨ä¸ºæ–°é•œåƒ
docker commit chip-dev chip-inference:v1.1

# å¯¼å‡º/å¯¼å…¥é•œåƒ (ç¦»çº¿éƒ¨ç½²)
docker save chip-inference:v1.0 > chip-inference.tar
docker load < chip-inference.tar

# æ¸…ç†
docker system prune  # æ¸…ç†æœªä½¿ç”¨çš„èµ„æº
docker system prune -a  # æ¸…ç†æ‰€æœ‰æœªä½¿ç”¨çš„é•œåƒ
docker volume prune  # æ¸…ç†æœªä½¿ç”¨çš„å·
```

---

### 2.3 C/C++åŸºç¡€é˜…è¯»èƒ½åŠ›

#### ğŸ“š ä¸ºä»€ä¹ˆéœ€è¦C++?

```
AIèŠ¯ç‰‡SDKé€šå¸¸ç”¨C++ç¼–å†™:
â”œâ”€â”€ é©±åŠ¨å±‚: C/C++
â”œâ”€â”€ ç®—å­åº“: CUDA C++, HIP C++
â”œâ”€â”€ Pythonç»‘å®š: pybind11 (C++)
â””â”€â”€ é…ç½®æ„å»º: CMake

ä½ éœ€è¦èƒ½å¤Ÿ:
âœ… é˜…è¯»SDKæ–‡æ¡£å’Œç¤ºä¾‹ä»£ç 
âœ… çœ‹æ‡‚æŠ¥é”™ä¿¡æ¯
âœ… ä¿®æ”¹CMakeLists.txt
âœ… ç¼–è¯‘ç®€å•çš„C++æ‰©å±•
```

---

#### ğŸ”§ å¿…å¤‡åŸºç¡€çŸ¥è¯†

**æŒ‡é’ˆä¸å¼•ç”¨**

```cpp
// æŒ‡é’ˆ: å­˜å‚¨åœ°å€çš„å˜é‡
int x = 10;
int* ptr = &x;  // ptrå­˜å‚¨xçš„åœ°å€
*ptr = 20;      // é€šè¿‡æŒ‡é’ˆä¿®æ”¹xçš„å€¼

// å¼•ç”¨: åˆ«å
int& ref = x;   // refæ˜¯xçš„åˆ«å
ref = 30;       // ä¿®æ”¹refå°±æ˜¯ä¿®æ”¹x

// æ™ºèƒ½æŒ‡é’ˆ (ç°ä»£C++)
#include <memory>

// unique_ptr: ç‹¬å æ‰€æœ‰æƒ
std::unique_ptr<Model> model = std::make_unique<Model>();

// shared_ptr: å…±äº«æ‰€æœ‰æƒ
std::shared_ptr<Tensor> tensor = std::make_shared<Tensor>(shape);

// ä¸ºä»€ä¹ˆé‡è¦:
// - è‡ªåŠ¨å†…å­˜ç®¡ç†,é˜²æ­¢æ³„æ¼
// - SDKè¿”å›å€¼å¸¸ç”¨æ™ºèƒ½æŒ‡é’ˆ
```

**ç±»ä¸å¯¹è±¡**

```cpp
// å…¸å‹çš„SDKç±»ç»“æ„
class InferenceEngine {
public:
    // æ„é€ å‡½æ•°
    InferenceEngine(const std::string& model_path);
    
    // ææ„å‡½æ•° (æ¸…ç†èµ„æº)
    ~InferenceEngine();
    
    // æˆå‘˜å‡½æ•°
    Tensor forward(const Tensor& input);
    void set_device(int device_id);
    
private:
    // æˆå‘˜å˜é‡
    std::shared_ptr<Model> model_;
    int device_id_;
};

// ä½¿ç”¨
InferenceEngine engine("/models/chatglm.onnx");
engine.set_device(0);
Tensor output = engine.forward(input_tensor);
```

**å¤´æ–‡ä»¶ä¸ç¼–è¯‘**

```cpp
// model.h (å¤´æ–‡ä»¶ - å£°æ˜)
#ifndef MODEL_H
#define MODEL_H

class Model {
public:
    void load(const char* path);
    float* inference(float* input, int size);
};

#endif

// model.cpp (å®ç°æ–‡ä»¶)
#include "model.h"
#include <iostream>

void Model::load(const char* path) {
    std::cout << "Loading model from " << path << std::endl;
    // å®ç°...
}

float* Model::inference(float* input, int size) {
    // å®ç°...
    return nullptr;
}
```

---

#### âš™ï¸ CMakeåŸºç¡€

**CMakeLists.txtç¤ºä¾‹**

```cmake
# æœ€ä½CMakeç‰ˆæœ¬
cmake_minimum_required(VERSION 3.18)

# é¡¹ç›®åç§°
project(ChipInference LANGUAGES CXX CUDA)

# C++æ ‡å‡†
set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# æŸ¥æ‰¾ä¾èµ–
find_package(CUDA REQUIRED)
find_package(Python3 COMPONENTS Interpreter Development REQUIRED)

# åŒ…å«ç›®å½•
include_directories(
    ${CMAKE_CURRENT_SOURCE_DIR}/include
    ${CUDA_INCLUDE_DIRS}
    ${Python3_INCLUDE_DIRS}
)

# æºæ–‡ä»¶
set(SOURCES
    src/model.cpp
    src/inference.cu  # CUDAæ–‡ä»¶
)

# ç¼–è¯‘åº“
add_library(chip_inference SHARED ${SOURCES})

# é“¾æ¥åº“
target_link_libraries(chip_inference
    ${CUDA_LIBRARIES}
    ${Python3_LIBRARIES}
)

# å®‰è£…
install(TARGETS chip_inference DESTINATION lib)
install(FILES include/model.h DESTINATION include)
```

**ç¼–è¯‘æµç¨‹**

```bash
# åˆ›å»ºæ„å»ºç›®å½•
mkdir build && cd build

# ç”ŸæˆMakefile
cmake ..

# ç¼–è¯‘
make -j8  # 8çº¿ç¨‹å¹¶è¡Œç¼–è¯‘

# å®‰è£…
sudo make install

# å¸¸è§é—®é¢˜æ’æŸ¥
# 1. æ‰¾ä¸åˆ°CUDA
cmake .. -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-11.8

# 2. Pythonç‰ˆæœ¬ä¸å¯¹
cmake .. -DPython3_EXECUTABLE=/usr/bin/python3.10

# 3. æŸ¥çœ‹è¯¦ç»†ç¼–è¯‘è¿‡ç¨‹
make VERBOSE=1
```

---

#### ğŸ pybind11 - Python/C++ç»‘å®š

**ä¸ºä»€ä¹ˆé‡è¦**: èŠ¯ç‰‡SDKé€šå¸¸æä¾›C++ API,éœ€è¦ç»‘å®šåˆ°Python

```cpp
// binding.cpp
#include <pybind11/pybind11.h>
#include <pybind11/numpy.h>
#include "model.h"

namespace py = pybind11;

PYBIND11_MODULE(chip_inference, m) {
    m.doc() = "AI Chip Inference Module";
    
    // ç»‘å®šç±»
    py::class_<Model>(m, "Model")
        .def(py::init<const std::string&>())  // æ„é€ å‡½æ•°
        .def("load", &Model::load)
        .def("inference", &Model::inference)
        .def("get_device_id", &Model::get_device_id);
    
    // ç»‘å®šå‡½æ•°
    m.def("get_device_count", &get_device_count, "Get number of devices");
}
```

**Pythonä½¿ç”¨**

```python
import chip_inference

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = chip_inference.Model("/models/chatglm.onnx")

# è°ƒç”¨C++æ–¹æ³•
model.load()
output = model.inference(input_array)

# æŸ¥è¯¢è®¾å¤‡
device_count = chip_inference.get_device_count()
print(f"å¯ç”¨è®¾å¤‡æ•°: {device_count}")
```

---

#### ğŸ” é˜…è¯»SDKæ–‡æ¡£æŠ€å·§

**å…¸å‹SDKæ–‡æ¡£ç»“æ„**

```
chip-sdk/
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ QuickStart.md        # å¿«é€Ÿå¼€å§‹
â”‚   â”œâ”€â”€ API_Reference.md     # APIæ–‡æ¡£
â”‚   â””â”€â”€ Troubleshooting.md   # æ•…éšœæ’æŸ¥
â”œâ”€â”€ include/                 # å¤´æ–‡ä»¶
â”‚   â””â”€â”€ chip_runtime.h
â”œâ”€â”€ lib/                     # åº“æ–‡ä»¶
â”‚   â””â”€â”€ libchip_runtime.so
â”œâ”€â”€ examples/                # ç¤ºä¾‹ä»£ç  â­
â”‚   â”œâ”€â”€ cpp/
â”‚   â””â”€â”€ python/
â””â”€â”€ tools/
    â””â”€â”€ chip-smi            # ç›‘æ§å·¥å…·
```

**é˜…è¯»ç­–ç•¥**

```
1. å…ˆçœ‹QuickStart
   - å¤åˆ¶æœ€ç®€å•çš„ç¤ºä¾‹
   - ç¡®ä¿èƒ½ç¼–è¯‘è¿è¡Œ

2. å‚è€ƒexamples/
   - æ‰¾ç›¸ä¼¼åœºæ™¯çš„ä»£ç 
   - ç†è§£åŸºæœ¬æµç¨‹:
     åˆå§‹åŒ– -> åŠ è½½æ¨¡å‹ -> æ¨ç† -> æ¸…ç†

3. é‡åˆ°é”™è¯¯æ—¶çœ‹API Reference
   - æœç´¢å‡½æ•°å
   - ç†è§£å‚æ•°å«ä¹‰
   - æŸ¥çœ‹è¿”å›å€¼

4. æŠ¥é”™æ—¶çœ‹Troubleshooting
   - æœç´¢é”™è¯¯ç 
   - æŸ¥çœ‹å¸¸è§é—®é¢˜
```

**å®ä¾‹: ç†è§£SDKåˆå§‹åŒ–ä»£ç **

```cpp
// ä»SDKç¤ºä¾‹ä¸­ç†è§£
#include <chip_runtime.h>

int main() {
    // 1. åˆå§‹åŒ–è¿è¡Œæ—¶
    chipInit();  // â† æŸ¥æ–‡æ¡£: åˆå§‹åŒ–èŠ¯ç‰‡é©±åŠ¨
    
    // 2. è·å–è®¾å¤‡
    chipDevice_t device;
    chipGetDevice(&device, 0);  // â† å‚æ•°0æ˜¯è®¾å¤‡ID
    
    // 3. åˆ›å»ºä¸Šä¸‹æ–‡
    chipContext_t ctx;
    chipCtxCreate(&ctx, 0, device);  // â† flag=0è¡¨ç¤ºé»˜è®¤
    
    // 4. åˆ†é…æ˜¾å­˜
    void* d_ptr;
    chipMalloc(&d_ptr, 1024 * sizeof(float));  // â† ç±»ä¼¼cudaMalloc
    
    // 5. æ¸…ç†
    chipFree(d_ptr);
    chipCtxDestroy(ctx);
    
    return 0;
}

// Pythonç»‘å®šç‰ˆæœ¬ (æ¨æµ‹)
import chip_runtime as chip

chip.init()
device = chip.get_device(0)
ctx = chip.create_context(device)
d_ptr = chip.malloc(1024 * 4)  # 4 bytes per float
chip.free(d_ptr)
```

---

## ğŸ“ å­¦ä¹ è·¯çº¿

```infographic
infographic sequence-timeline-simple
data
  title ç³»ç»Ÿå·¥å…·é“¾å­¦ä¹ æ—¶é—´çº¿
  items
    - label ç¬¬1å‘¨
      desc LinuxåŸºç¡€å‘½ä»¤ + Shellè„šæœ¬
    - label ç¬¬2å‘¨
      desc DockeråŸºç¡€ + Dockerfile
    - label ç¬¬3å‘¨
      desc C++åŸºç¡€é˜…è¯» + CMake
    - label ç¬¬4å‘¨
      desc ç»¼åˆå®è·µï¼šéƒ¨ç½²é¡¹ç›®
```

---

**æœ¬èŠ‚æ€»ç»“**

âœ… **Linux**: å‘½ä»¤è¡Œã€è¿›ç¨‹ç®¡ç†ã€èµ„æºç›‘æ§
âœ… **Docker**: å®¹å™¨åŒ–ã€é•œåƒæ„å»ºã€Composeç¼–æ’
âœ… **C++**: åŸºç¡€è¯­æ³•ã€CMakeã€SDKé›†æˆ
âœ… **å®æˆ˜èƒ½åŠ›**: èƒ½å¤Ÿæ­å»ºéƒ¨ç½²ç¯å¢ƒã€è°ƒè¯•é—®é¢˜

**ä¸‹ä¸€èŠ‚**: åº”ç”¨å¼€å‘ - Pythonåç«¯ + é£ä¹¦é›†æˆ + RAGæŠ€æœ¯

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¶é—´**: 2026-01-08
