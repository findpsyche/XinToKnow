# åº”ç”¨å¼€å‘ä¸è½¯æŠ€èƒ½ - å°†æŠ€æœ¯è½¬åŒ–ä¸ºäº§å“

## ä¸‰ã€åº”ç”¨å¼€å‘ä¸é›†æˆ (App Dev & RAG)

---

### 3.1 Pythonåç«¯å¼€å‘ â­â­â­â­â­

#### ğŸš€ FastAPIæ·±åº¦å®æˆ˜

**ä¸ºä»€ä¹ˆé€‰æ‹©FastAPI?**

```
FastAPI vs Flask vs Django:

FastAPI:
âœ… å¼‚æ­¥æ”¯æŒ (å¤„ç†AIæ¨ç†æ…¢è¯·æ±‚)
âœ… è‡ªåŠ¨APIæ–‡æ¡£ (Swagger UI)
âœ… ç±»å‹æç¤º + æ•°æ®éªŒè¯ (Pydantic)
âœ… é«˜æ€§èƒ½ (åŸºäºStarlette + uvicorn)
âœ… ç°ä»£Pythonç‰¹æ€§

é€‚åˆåœºæ™¯: AIæœåŠ¡ã€é«˜å¹¶å‘API
```

**å®Œæ•´æ¨ç†æœåŠ¡ç¤ºä¾‹**

```python
# src/server.py
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, List
import asyncio
import time
from contextlib import asynccontextmanager

# ===== æ•°æ®æ¨¡å‹ =====
class GenerateRequest(BaseModel):
    prompt: str = Field(..., min_length=1, max_length=2048, 
                       description="è¾“å…¥æç¤º")
    max_tokens: int = Field(default=512, ge=1, le=2048)
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    top_p: float = Field(default=0.9, ge=0.0, le=1.0)
    stream: bool = Field(default=False, description="æµå¼è¾“å‡º")
    
    class Config:
        schema_extra = {
            "example": {
                "prompt": "è§£é‡Šä»€ä¹ˆæ˜¯Transformer",
                "max_tokens": 256,
                "temperature": 0.7,
                "stream": False
            }
        }

class GenerateResponse(BaseModel):
    text: str
    usage: dict
    latency: float

# ===== æ¨¡å‹ç®¡ç† =====
class ModelManager:
    """å•ä¾‹æ¨¡å¼çš„æ¨¡å‹ç®¡ç†å™¨"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.model = None
            self.tokenizer = None
            self.initialized = False
    
    async def load_model(self):
        """å¼‚æ­¥åŠ è½½æ¨¡å‹(é¿å…é˜»å¡å¯åŠ¨)"""
        if self.initialized:
            return
        
        print("åŠ è½½æ¨¡å‹...")
        from transformers import AutoModel, AutoTokenizer
        import torch
        
        model_name = "THUDM/chatglm3-6b"
        
        # å¼‚æ­¥æ‰§è¡Œè€—æ—¶æ“ä½œ
        loop = asyncio.get_event_loop()
        self.tokenizer = await loop.run_in_executor(
            None, 
            AutoTokenizer.from_pretrained, 
            model_name, 
            True  # trust_remote_code
        )
        
        self.model = await loop.run_in_executor(
            None,
            lambda: AutoModel.from_pretrained(
                model_name,
                trust_remote_code=True,
                device_map="auto"
            ).eval()
        )
        
        self.initialized = True
        print("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    async def generate(self, request: GenerateRequest):
        """æ¨ç†"""
        if not self.initialized:
            await self.load_model()
        
        start_time = time.time()
        
        # åœ¨çº¿ç¨‹æ± æ‰§è¡Œ(é¿å…é˜»å¡äº‹ä»¶å¾ªç¯)
        loop = asyncio.get_event_loop()
        response, history = await loop.run_in_executor(
            None,
            self._sync_generate,
            request
        )
        
        latency = time.time() - start_time
        
        return GenerateResponse(
            text=response,
            usage={
                "prompt_tokens": len(self.tokenizer.encode(request.prompt)),
                "completion_tokens": len(self.tokenizer.encode(response)),
            },
            latency=latency
        )
    
    def _sync_generate(self, request: GenerateRequest):
        """åŒæ­¥ç”Ÿæˆ(åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ)"""
        response, history = self.model.chat(
            self.tokenizer,
            request.prompt,
            history=[],
            max_length=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p
        )
        return response, history

# ===== FastAPIåº”ç”¨ =====
model_manager = ModelManager()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    await model_manager.load_model()
    yield
    # å…³é—­æ—¶æ¸…ç†èµ„æº
    print("æ¸…ç†èµ„æº...")

app = FastAPI(
    title="AIèŠ¯ç‰‡æ¨ç†API",
    description="åŸºäºè‡ªç ”èŠ¯ç‰‡çš„å¤§æ¨¡å‹æ¨ç†æœåŠ¡",
    version="1.0.0",
    lifespan=lifespan
)

# ===== è·¯ç”± =====
@app.get("/")
async def root():
    return {"message": "AIèŠ¯ç‰‡æ¨ç†æœåŠ¡è¿è¡Œä¸­"}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥(K8s/Dockerä½¿ç”¨)"""
    return {
        "status": "healthy",
        "model_loaded": model_manager.initialized
    }

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """ç”Ÿæˆæ–‡æœ¬"""
    try:
        return await model_manager.generate(request)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/generate/stream")
async def generate_stream(request: GenerateRequest):
    """æµå¼ç”Ÿæˆ"""
    async def event_generator():
        # æ¨¡æ‹Ÿæµå¼è¾“å‡º
        full_response = await model_manager.generate(request)
        words = full_response.text.split()
        
        for word in words:
            yield f"data: {word} \n\n"
            await asyncio.sleep(0.05)
        
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )

# ===== ä¸­é—´ä»¶ =====
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)

# è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
from starlette.middleware.base import BaseHTTPMiddleware
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
        
        logger.info(
            f"{request.method} {request.url.path} "
            f"status={response.status_code} "
            f"time={process_time:.3f}s"
        )
        
        return response

app.add_middleware(LoggingMiddleware)

# ===== è¿è¡Œ =====
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,  # å¼€å‘æ¨¡å¼
        workers=1     # ç”Ÿäº§ç¯å¢ƒå¯å¢åŠ 
    )
```

**è¿è¡Œä¸æµ‹è¯•**

```bash
# å¯åŠ¨æœåŠ¡
python server.py

# è®¿é—®APIæ–‡æ¡£
# http://localhost:8000/docs  (Swagger UI)
# http://localhost:8000/redoc (ReDoc)

# æµ‹è¯•API
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ä»€ä¹ˆæ˜¯AIèŠ¯ç‰‡?",
    "max_tokens": 100,
    "temperature": 0.7
  }'

# Pythonå®¢æˆ·ç«¯
import requests

response = requests.post(
    "http://localhost:8000/generate",
    json={
        "prompt": "ä»€ä¹ˆæ˜¯AIèŠ¯ç‰‡?",
        "max_tokens": 100
    }
)

print(response.json())
```

---

#### âš¡ å¼‚æ­¥ç¼–ç¨‹æ·±å…¥

**ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥?**

```python
# åŒæ­¥ç‰ˆæœ¬ (é˜»å¡)
def handle_request():
    result = model.generate(prompt)  # é˜»å¡5ç§’
    return result

# é—®é¢˜: å¤„ç†ä¸€ä¸ªè¯·æ±‚æ—¶,å…¶ä»–è¯·æ±‚ç­‰å¾…
# 10ä¸ªè¯·æ±‚ â†’ 10 Ã— 5ç§’ = 50ç§’

# å¼‚æ­¥ç‰ˆæœ¬ (éé˜»å¡)
async def handle_request():
    result = await model.generate(prompt)  # ç­‰å¾…æœŸé—´å¯å¤„ç†å…¶ä»–è¯·æ±‚
    return result

# ä¼˜åŠ¿: 10ä¸ªè¯·æ±‚å¹¶å‘ â†’ ~5ç§’å®Œæˆ
```

**async/awaitè¯¦è§£**

```python
import asyncio
import aiohttp
import time

# ===== åŸºç¡€ç¤ºä¾‹ =====
async def fetch_url(session, url):
    """å¼‚æ­¥HTTPè¯·æ±‚"""
    async with session.get(url) as response:
        return await response.text()

async def main():
    urls = [
        "https://api.example.com/model1",
        "https://api.example.com/model2",
        "https://api.example.com/model3",
    ]
    
    async with aiohttp.ClientSession() as session:
        # å¹¶å‘æ‰§è¡Œ
        tasks = [fetch_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
    
    return results

# è¿è¡Œ
results = asyncio.run(main())

# ===== å®é™…åº”ç”¨: æ‰¹é‡æ¨ç† =====
async def batch_inference(prompts: List[str]):
    """æ‰¹é‡æ¨ç†"""
    tasks = [model_manager.generate(prompt) for prompt in prompts]
    return await asyncio.gather(*tasks)

# ===== è¶…æ—¶æ§åˆ¶ =====
async def inference_with_timeout(prompt, timeout=30.0):
    """å¸¦è¶…æ—¶çš„æ¨ç†"""
    try:
        result = await asyncio.wait_for(
            model_manager.generate(prompt),
            timeout=timeout
        )
        return result
    except asyncio.TimeoutError:
        raise HTTPException(
            status_code=408,
            detail="æ¨ç†è¶…æ—¶"
        )

# ===== ä»»åŠ¡é˜Ÿåˆ— =====
from asyncio import Queue

class InferenceQueue:
    def __init__(self, max_workers=4):
        self.queue = Queue()
        self.workers = max_workers
    
    async def worker(self):
        """å·¥ä½œåç¨‹"""
        while True:
            request = await self.queue.get()
            try:
                result = await model_manager.generate(request)
                request.callback(result)
            finally:
                self.queue.task_done()
    
    async def start(self):
        """å¯åŠ¨workers"""
        for _ in range(self.workers):
            asyncio.create_task(self.worker())
    
    async def submit(self, request, callback):
        """æäº¤ä»»åŠ¡"""
        request.callback = callback
        await self.queue.put(request)
```

---

### 3.2 é£ä¹¦å¼€æ”¾å¹³å°å¼€å‘ â­â­â­â­â­

#### ğŸ“± é£ä¹¦å¹³å°æ¶æ„ç†è§£

```
é£ä¹¦åº”ç”¨é€šä¿¡æµç¨‹:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç”¨æˆ·    â”‚ â”€â”€æ¶ˆæ¯â”€â†’ â”‚ é£ä¹¦æœåŠ¡å™¨    â”‚ â”€â”€å›è°ƒâ”€â†’ â”‚ ä½ çš„æœåŠ¡â”‚
â”‚         â”‚ â†â”€å›å¤â”€â”€ â”‚              â”‚ â†â”€APIâ”€â”€  â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                    äº‹ä»¶è®¢é˜… (Webhook)
                    æ¶ˆæ¯APIè°ƒç”¨
```

---

#### ğŸ” OAuth 2.0 é‰´æƒè¯¦è§£

**è·å–Access Token**

```python
# auth.py
import requests
import time
from typing import Optional

class FeishuAuth:
    """é£ä¹¦é‰´æƒç®¡ç†å™¨"""
    
    def __init__(self, app_id: str, app_secret: str):
        self.app_id = app_id
        self.app_secret = app_secret
        self._token = None
        self._expire_time = 0
    
    def get_tenant_access_token(self) -> str:
        """è·å–tenant_access_token"""
        # æ£€æŸ¥ç¼“å­˜
        if self._token and time.time() < self._expire_time:
            return self._token
        
        # è¯·æ±‚æ–°token
        url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
        headers = {"Content-Type": "application/json"}
        data = {
            "app_id": self.app_id,
            "app_secret": self.app_secret
        }
        
        response = requests.post(url, json=data, headers=headers)
        result = response.json()
        
        if result["code"] != 0:
            raise Exception(f"è·å–tokenå¤±è´¥: {result}")
        
        # ç¼“å­˜token (æå‰5åˆ†é’Ÿè¿‡æœŸ)
        self._token = result["tenant_access_token"]
        self._expire_time = time.time() + result["expire"] - 300
        
        return self._token
    
    def get_headers(self) -> dict:
        """è·å–å¸¦é‰´æƒçš„è¯·æ±‚å¤´"""
        return {
            "Authorization": f"Bearer {self.get_tenant_access_token()}",
            "Content-Type": "application/json"
        }

# ä½¿ç”¨
auth = FeishuAuth(
    app_id="cli_xxxxx",
    app_secret="xxxxxx"
)

headers = auth.get_headers()
```

---

#### ğŸ“¨ äº‹ä»¶è®¢é˜…ä¸æ¶ˆæ¯å¤„ç†

**å®Œæ•´çš„é£ä¹¦Bot**

```python
# feishu_bot.py
from fastapi import FastAPI, Request, HTTPException
from pydantic import BaseModel
import hashlib
import json
from typing import Optional

app = FastAPI()

# é…ç½®
VERIFICATION_TOKEN = "your_verification_token"
ENCRYPT_KEY = "your_encrypt_key"  # å¦‚æœå¯ç”¨åŠ å¯†

# ===== äº‹ä»¶è§£å¯† =====
from Crypto.Cipher import AES
import base64

def decrypt_message(encrypt: str) -> str:
    """è§£å¯†é£ä¹¦æ¶ˆæ¯"""
    cipher = AES.new(ENCRYPT_KEY.encode(), AES.MODE_CBC, ENCRYPT_KEY.encode())
    decrypted = cipher.decrypt(base64.b64decode(encrypt))
    # å»é™¤padding
    return decrypted[:-decrypted[-1]].decode()

# ===== Webhookå¤„ç† =====
@app.post("/feishu/webhook")
async def feishu_webhook(request: Request):
    """æ¥æ”¶é£ä¹¦äº‹ä»¶"""
    body = await request.json()
    
    # 1. URLéªŒè¯
    if "challenge" in body:
        return {"challenge": body["challenge"]}
    
    # 2. TokenéªŒè¯
    token = body.get("header", {}).get("token")
    if token != VERIFICATION_TOKEN:
        raise HTTPException(status_code=403, detail="TokenéªŒè¯å¤±è´¥")
    
    # 3. è§£å¯†(å¦‚æœå¯ç”¨)
    if "encrypt" in body:
        decrypted = decrypt_message(body["encrypt"])
        body = json.loads(decrypted)
    
    # 4. å¤„ç†äº‹ä»¶
    event_type = body.get("header", {}).get("event_type")
    
    if event_type == "im.message.receive_v1":
        await handle_message(body)
    elif event_type == "im.message.reaction.created_v1":
        await handle_reaction(body)
    
    return {"code": 0}

# ===== æ¶ˆæ¯å¤„ç† =====
async def handle_message(event: dict):
    """å¤„ç†æ¥æ”¶åˆ°çš„æ¶ˆæ¯"""
    message = event["event"]["message"]
    sender = event["event"]["sender"]
    
    # è§£ææ¶ˆæ¯å†…å®¹
    content = json.loads(message["content"])
    
    # è¿‡æ»¤Botè‡ªå·±çš„æ¶ˆæ¯
    if sender["sender_type"] == "app":
        return
    
    # æå–æ–‡æœ¬
    if message["message_type"] == "text":
        user_input = content["text"]
        print(f"æ”¶åˆ°æ¶ˆæ¯: {user_input}")
        
        # è°ƒç”¨AIç”Ÿæˆå›å¤
        ai_response = await generate_ai_response(user_input)
        
        # å‘é€å›å¤
        await send_message(
            message["chat_id"],
            ai_response,
            message_type="text"
        )
    
    # å¤„ç†å›¾ç‰‡æ¶ˆæ¯
    elif message["message_type"] == "image":
        image_key = content["image_key"]
        # ä¸‹è½½å›¾ç‰‡å¹¶å¤„ç†...

async def generate_ai_response(user_input: str) -> str:
    """è°ƒç”¨AIæ¨¡å‹ç”Ÿæˆå›å¤"""
    # é›†æˆå‰é¢çš„æ¨ç†æœåŠ¡
    import httpx
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/generate",
            json={"prompt": user_input, "max_tokens": 200}
        )
        result = response.json()
        return result["text"]

# ===== å‘é€æ¶ˆæ¯ =====
async def send_message(
    chat_id: str,
    content: str,
    message_type: str = "text"
):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦"""
    url = "https://open.feishu.cn/open-apis/im/v1/messages"
    
    params = {"receive_id_type": "chat_id"}
    headers = auth.get_headers()
    
    # æ„é€ æ¶ˆæ¯ä½“
    if message_type == "text":
        message_content = {"text": content}
    elif message_type == "post":  # å¯Œæ–‡æœ¬
        message_content = content  # åº”è¯¥æ˜¯å¤æ‚çš„JSONç»“æ„
    
    data = {
        "receive_id": chat_id,
        "msg_type": message_type,
        "content": json.dumps(message_content)
    }
    
    import httpx
    async with httpx.AsyncClient() as client:
        response = await client.post(url, params=params, headers=headers, json=data)
        result = response.json()
        
        if result["code"] != 0:
            print(f"å‘é€æ¶ˆæ¯å¤±è´¥: {result}")

# ===== è¡¨æƒ…å›åº”å¤„ç† =====
async def handle_reaction(event: dict):
    """å¤„ç†è¡¨æƒ…å›åº”"""
    reaction = event["event"]["reaction_type"]
    message_id = event["event"]["message_id"]
    
    print(f"æ¶ˆæ¯ {message_id} æ”¶åˆ°è¡¨æƒ…: {reaction}")
```

---

#### ğŸ¨ é£ä¹¦å¡ç‰‡ (Interactive Card)

**æ„å»ºäº¤äº’å¼å¡ç‰‡**

```python
def create_ai_response_card(question: str, answer: str) -> dict:
    """åˆ›å»ºAIå›ç­”å¡ç‰‡"""
    return {
        "config": {
            "wide_screen_mode": True
        },
        "header": {
            "template": "blue",
            "title": {
                "content": "ğŸ¤– AIåŠ©æ‰‹å›ç­”",
                "tag": "plain_text"
            }
        },
        "elements": [
            # é—®é¢˜éƒ¨åˆ†
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"**æ‚¨çš„é—®é¢˜:**\n{question}"
                }
            },
            {
                "tag": "hr"
            },
            # å›ç­”éƒ¨åˆ†
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"**AIå›ç­”:**\n{answer}"
                }
            },
            # æ“ä½œæŒ‰é’®
            {
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {
                            "tag": "plain_text",
                            "content": "ğŸ‘ æœ‰å¸®åŠ©"
                        },
                        "type": "primary",
                        "value": {"action": "thumbs_up"}
                    },
                    {
                        "tag": "button",
                        "text": {
                            "tag": "plain_text",
                            "content": "ğŸ‘ æ— å¸®åŠ©"
                        },
                        "type": "default",
                        "value": {"action": "thumbs_down"}
                    },
                    {
                        "tag": "button",
                        "text": {
                            "tag": "plain_text",
                            "content": "ğŸ”„ é‡æ–°ç”Ÿæˆ"
                        },
                        "type": "default",
                        "value": {"action": "regenerate", "prompt": question}
                    }
                ]
            }
        ]
    }

# å‘é€å¡ç‰‡
async def send_card(chat_id: str, card_content: dict):
    """å‘é€äº¤äº’å¼å¡ç‰‡"""
    await send_message(
        chat_id,
        json.dumps(card_content),
        message_type="interactive"
    )

# å¤„ç†å¡ç‰‡äº¤äº’
@app.post("/feishu/card_callback")
async def card_callback(request: Request):
    """å¤„ç†å¡ç‰‡æŒ‰é’®ç‚¹å‡»"""
    body = await request.json()
    
    action = body["action"]["value"]["action"]
    
    if action == "thumbs_up":
        # è®°å½•ç”¨æˆ·åé¦ˆ
        print("ç”¨æˆ·ç‚¹èµ")
    elif action == "regenerate":
        prompt = body["action"]["value"]["prompt"]
        # é‡æ–°ç”Ÿæˆ...
    
    return {"code": 0}
```

---

### 3.3 RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) â­â­â­â­â­

#### ğŸ§  RAGæ¶æ„ç†è§£

```
ä¼ ç»ŸLLM:
ç”¨æˆ·é—®é¢˜ â†’ LLM â†’ å›ç­”
é—®é¢˜: åªèƒ½ä¾èµ–è®­ç»ƒæ•°æ®,æ— æ³•è·å–æœ€æ–°ä¿¡æ¯

RAGæµç¨‹:
ç”¨æˆ·é—®é¢˜ â†’ Embedding â†’ å‘é‡æ£€ç´¢ â†’ ç›¸å…³æ–‡æ¡£
           â†“                              â†“
        LLM â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç»„åˆPrompt â†â”€â”€â”€â”€â”€â”€â”˜
           â†“
        å›ç­”

ä¼˜åŠ¿:
âœ… å¼•å…¥å¤–éƒ¨çŸ¥è¯†åº“
âœ… å¯æ›´æ–°(ä¸éœ€é‡æ–°è®­ç»ƒ)
âœ… å¯æº¯æº(çŸ¥é“ç­”æ¡ˆæ¥è‡ªå“ªé‡Œ)
âœ… é™ä½å¹»è§‰
```

---

#### ğŸ”¢ Embeddingè¯¦è§£

**ä»€ä¹ˆæ˜¯Embedding?**

```python
# æ–‡æœ¬ â†’ å‘é‡

text1 = "äººå·¥æ™ºèƒ½"
embedding1 = [0.23, -0.15, 0.67, ..., 0.42]  # 768ç»´å‘é‡

text2 = "AIæŠ€æœ¯"
embedding2 = [0.25, -0.14, 0.65, ..., 0.40]  # è¯­ä¹‰ç›¸è¿‘,å‘é‡æ¥è¿‘

# ç›¸ä¼¼åº¦è®¡ç®—
from numpy import dot
from numpy.linalg import norm

def cosine_similarity(a, b):
    return dot(a, b) / (norm(a) * norm(b))

sim = cosine_similarity(embedding1, embedding2)
# sim â‰ˆ 0.95 (éå¸¸ç›¸ä¼¼)
```

**ä½¿ç”¨Embeddingæ¨¡å‹**

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½ä¸­æ–‡embeddingæ¨¡å‹
model = SentenceTransformer('BAAI/bge-small-zh-v1.5')

# ç¼–ç æ–‡æœ¬
texts = [
    "AIèŠ¯ç‰‡åº”ç”¨å¼€å‘",
    "å¤§æ¨¡å‹éƒ¨ç½²ä¼˜åŒ–",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
]

embeddings = model.encode(texts)
print(embeddings.shape)  # (3, 512)

# è®¡ç®—ç›¸ä¼¼åº¦
from scipy.spatial.distance import cosine

sim_01 = 1 - cosine(embeddings[0], embeddings[1])
sim_02 = 1 - cosine(embeddings[0], embeddings[2])

print(f"å¥å­0å’Œ1çš„ç›¸ä¼¼åº¦: {sim_01:.3f}")  # 0.782 (ç›¸å…³)
print(f"å¥å­0å’Œ2çš„ç›¸ä¼¼åº¦: {sim_02:.3f}")  # 0.123 (ä¸ç›¸å…³)
```

---

#### ğŸ—„ï¸ å‘é‡æ•°æ®åº“å®æˆ˜

**ä½¿ç”¨FAISS (æ¨èå¼€å‘)**

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

class FAISSVectorStore:
    """FAISSå‘é‡å­˜å‚¨"""
    
    def __init__(self, embedding_model="BAAI/bge-small-zh-v1.5"):
        self.model = SentenceTransformer(embedding_model)
        self.dimension = self.model.get_sentence_embedding_dimension()
        
        # åˆ›å»ºç´¢å¼•
        self.index = faiss.IndexFlatIP(self.dimension)  # å†…ç§¯ç´¢å¼•
        # æˆ–ä½¿ç”¨æ›´å¿«çš„ç´¢å¼•: faiss.IndexIVFFlat()
        
        self.texts = []  # å­˜å‚¨åŸæ–‡
    
    def add_texts(self, texts: List[str]):
        """æ·»åŠ æ–‡æ¡£"""
        # ç¼–ç 
        embeddings = self.model.encode(texts, normalize_embeddings=True)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        self.index.add(embeddings.astype('float32'))
        
        # ä¿å­˜åŸæ–‡
        self.texts.extend(texts)
    
    def search(self, query: str, top_k: int = 3):
        """æ£€ç´¢"""
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.model.encode([query], normalize_embeddings=True)
        
        # æœç´¢
        scores, indices = self.index.search(
            query_embedding.astype('float32'), 
            top_k
        )
        
        # è¿”å›ç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            results.append({
                "text": self.texts[idx],
                "score": float(score)
            })
        
        return results
    
    def save(self, path: str):
        """ä¿å­˜ç´¢å¼•"""
        faiss.write_index(self.index, f"{path}/index.faiss")
        
        import json
        with open(f"{path}/texts.json", "w") as f:
            json.dump(self.texts, f, ensure_ascii=False)
    
    def load(self, path: str):
        """åŠ è½½ç´¢å¼•"""
        self.index = faiss.read_index(f"{path}/index.faiss")
        
        import json
        with open(f"{path}/texts.json") as f:
            self.texts = json.load(f)

# ä½¿ç”¨ç¤ºä¾‹
vector_store = FAISSVectorStore()

# æ·»åŠ çŸ¥è¯†åº“
documents = [
    "AIèŠ¯ç‰‡æ˜¯ä¸“é—¨ä¸ºäººå·¥æ™ºèƒ½è®¡ç®—è®¾è®¡çš„å¤„ç†å™¨",
    "é‡åŒ–æŠ€æœ¯å¯ä»¥å°†FP32æ¨¡å‹å‹ç¼©åˆ°INT8,èŠ‚çœæ˜¾å­˜",
    "vLLMä½¿ç”¨PagedAttentionç®—æ³•æå‡æ¨ç†ååé‡",
    # ... æ›´å¤šæ–‡æ¡£
]

vector_store.add_texts(documents)

# æ£€ç´¢
query = "å¦‚ä½•ä¼˜åŒ–æ¨¡å‹æ¨ç†é€Ÿåº¦?"
results = vector_store.search(query, top_k=2)

for result in results:
    print(f"[{result['score']:.3f}] {result['text']}")
# [0.856] vLLMä½¿ç”¨PagedAttentionç®—æ³•æå‡æ¨ç†ååé‡
# [0.742] é‡åŒ–æŠ€æœ¯å¯ä»¥å°†FP32æ¨¡å‹å‹ç¼©åˆ°INT8,èŠ‚çœæ˜¾å­˜
```

**ä½¿ç”¨Milvus (æ¨èç”Ÿäº§)**

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

# è¿æ¥Milvus
connections.connect("default", host="localhost", port="19530")

# å®šä¹‰schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=512),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535)
]

schema = CollectionSchema(fields, description="çŸ¥è¯†åº“")
collection = Collection("knowledge_base", schema)

# åˆ›å»ºç´¢å¼•
index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "IP",  # å†…ç§¯
    "params": {"nlist": 1024}
}
collection.create_index("embedding", index_params)

# æ’å…¥æ•°æ®
entities = [
    embeddings.tolist(),  # å‘é‡åˆ—è¡¨
    texts                 # æ–‡æœ¬åˆ—è¡¨
]
collection.insert(entities)

# æœç´¢
search_params = {"metric_type": "IP", "params": {"nprobe": 10}}
results = collection.search(
    data=[query_embedding],
    anns_field="embedding",
    param=search_params,
    limit=3,
    output_fields=["text"]
)
```

---

#### âœ‚ï¸ æ–‡æ¡£åˆ‡ç‰‡ (Chunking)

**ä¸ºä»€ä¹ˆéœ€è¦åˆ‡ç‰‡?**

```
é•¿æ–‡æ¡£é—®é¢˜:
- ä¸€ç¯‡10000å­—çš„æ–‡æ¡£
- Embeddingæ¨¡å‹é™åˆ¶512 tokens
- æ£€ç´¢ç²’åº¦å¤ªç²—,æ— æ³•ç²¾å‡†åŒ¹é…

è§£å†³: åˆ‡åˆ†æˆå°å—
10000å­— â†’ 20ä¸ª500å­—çš„chunk
æ£€ç´¢æ—¶åŒ¹é…æœ€ç›¸å…³çš„2-3ä¸ªchunk
```

**åˆ‡ç‰‡ç­–ç•¥**

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# é€’å½’åˆ‡åˆ†å™¨ (æ¨è)
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # æ¯å—500å­—ç¬¦
    chunk_overlap=50,      # é‡å 50å­—ç¬¦(ä¿æŒä¸Šä¸‹æ–‡)
    separators=["\n\n", "\n", "ã€‚", "!", "?", "ï¼›", ",", " "]
)

document = """
AIèŠ¯ç‰‡æ˜¯ä¸“é—¨ä¸ºäººå·¥æ™ºèƒ½è®¡ç®—è®¾è®¡çš„å¤„ç†å™¨ã€‚ä¸é€šç”¨å¤„ç†å™¨ç›¸æ¯”,
AIèŠ¯ç‰‡é’ˆå¯¹æ·±åº¦å­¦ä¹ ç®—æ³•è¿›è¡Œäº†ä¼˜åŒ–,å…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚

ä¸»è¦ç±»å‹åŒ…æ‹¬:
1. GPU: é€šç”¨å›¾å½¢å¤„ç†å™¨
2. TPU: è°·æ­Œå¼ é‡å¤„ç†å™¨
3. NPU: ç¥ç»ç½‘ç»œå¤„ç†å™¨

... (å¾ˆé•¿çš„æ–‡æ¡£)
"""

chunks = splitter.split_text(document)

for i, chunk in enumerate(chunks):
    print(f"=== Chunk {i+1} ({len(chunk)}å­—ç¬¦) ===")
    print(chunk[:100])  # æ‰“å°å‰100å­—
    print()

# å¸¦å…ƒæ•°æ®çš„åˆ‡åˆ†
from langchain.docstore.document import Document

doc = Document(
    page_content=document,
    metadata={"source": "AIèŠ¯ç‰‡ç™½çš®ä¹¦", "page": 1}
)

chunks_with_metadata = splitter.split_documents([doc])

for chunk in chunks_with_metadata:
    print(chunk.page_content)
    print(chunk.metadata)  # ä¿ç•™å…ƒæ•°æ®
```

---

#### ğŸ”— å®Œæ•´RAGæµç¨‹

```python
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader, TextLoader

class RAGSystem:
    """å®Œæ•´çš„RAGç³»ç»Ÿ"""
    
    def __init__(self, knowledge_base_path: str):
        # 1. åŠ è½½æ–‡æ¡£
        loader = DirectoryLoader(
            knowledge_base_path,
            glob="**/*.md",
            loader_cls=TextLoader
        )
        documents = loader.load()
        
        # 2. åˆ‡ç‰‡
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        chunks = text_splitter.split_documents(documents)
        
        # 3. åˆ›å»ºå‘é‡åº“
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-small-zh-v1.5"
        )
        self.vector_store = FAISS.from_documents(chunks, embeddings)
        
        # 4. LLM (ä½¿ç”¨å‰é¢çš„æ¨ç†æœåŠ¡)
        self.llm_api = "http://localhost:8000/generate"
    
    async def answer_question(self, question: str) -> dict:
        """å›ç­”é—®é¢˜"""
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.vector_store.similarity_search(
            question,
            k=3
        )
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        # 3. æ„å»ºprompt
        prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”é—®é¢˜:

å‚è€ƒä¿¡æ¯:
{context}

é—®é¢˜: {question}

è¯·åŸºäºå‚è€ƒä¿¡æ¯ç»™å‡ºå‡†ç¡®çš„å›ç­”ã€‚å¦‚æœå‚è€ƒä¿¡æ¯ä¸­æ²¡æœ‰ç­”æ¡ˆ,è¯·è¯´æ˜"æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”æ­¤é—®é¢˜"ã€‚

å›ç­”:"""
        
        # 4. è°ƒç”¨LLM
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.post(
                self.llm_api,
                json={"prompt": prompt, "max_tokens": 300}
            )
            result = response.json()
        
        # 5. è¿”å›ç»“æœ(åŒ…å«æ¥æº)
        return {
            "answer": result["text"],
            "sources": [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in relevant_docs
            ],
            "confidence": self._calculate_confidence(relevant_docs)
        }
    
    def _calculate_confidence(self, docs):
        """è®¡ç®—ç½®ä¿¡åº¦"""
        # ç®€å•å®ç°: åŸºäºæ£€ç´¢åˆ†æ•°
        if len(docs) == 0:
            return 0.0
        # å®é™…åº”è¯¥ä½¿ç”¨æ£€ç´¢åˆ†æ•°
        return 0.8  # å ä½

# ä½¿ç”¨
rag_system = RAGSystem("/path/to/knowledge_base")
result = await rag_system.answer_question("å¦‚ä½•ä¼˜åŒ–å¤§æ¨¡å‹æ¨ç†?")

print(result["answer"])
print("\næ¥æº:")
for source in result["sources"]:
    print(f"- {source['metadata']['source']}")
```

---

## å››ã€è½¯æŠ€èƒ½ - æ–‡æ¡£ä¸æµ‹è¯•

### 4.1 æŠ€æœ¯å†™ä½œ â­â­â­â­

**APIæ–‡æ¡£æ¨¡æ¿**

```markdown
# æ¨¡å‹æ¨ç†APIæ–‡æ¡£

## æ¦‚è¿°
æœ¬APIæä¾›å¤§æ¨¡å‹æ¨ç†æœåŠ¡,æ”¯æŒæ–‡æœ¬ç”Ÿæˆã€‚

## åŸºç¡€ä¿¡æ¯
- Base URL: `https://api.example.com/v1`
- é‰´æƒæ–¹å¼: Bearer Token
- è¯·æ±‚æ ¼å¼: JSON
- å“åº”æ ¼å¼: JSON

## æ¥å£åˆ—è¡¨

### 1. ç”Ÿæˆæ–‡æœ¬
ç”Ÿæˆæ–‡æœ¬è¡¥å…¨æˆ–å¯¹è¯å›å¤ã€‚

**è¯·æ±‚**
```http
POST /generate
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "prompt": "ä»€ä¹ˆæ˜¯AIèŠ¯ç‰‡?",
  "max_tokens": 100,
  "temperature": 0.7
}
```

**å‚æ•°è¯´æ˜**

| å‚æ•° | ç±»å‹ | å¿…å¡« | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|------|
| prompt | string | æ˜¯ | è¾“å…¥æç¤º | "è§£é‡Šé‡å­è®¡ç®—" |
| max_tokens | integer | å¦ | æœ€å¤§ç”Ÿæˆtokenæ•° | 100 (é»˜è®¤512) |
| temperature | float | å¦ | æ§åˆ¶éšæœºæ€§ | 0.7 (èŒƒå›´0-2) |
| top_p | float | å¦ | æ ¸é‡‡æ ·å‚æ•° | 0.9 |

**å“åº”ç¤ºä¾‹**
```json
{
  "text": "AIèŠ¯ç‰‡æ˜¯ä¸“é—¨ä¸ºäººå·¥æ™ºèƒ½...",
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 87,
    "total_tokens": 102
  },
  "latency": 1.23
}
```

**é”™è¯¯ç **

| çŠ¶æ€ç  | è¯´æ˜ | è§£å†³æ–¹æ³• |
|--------|------|----------|
| 400 | å‚æ•°é”™è¯¯ | æ£€æŸ¥è¯·æ±‚å‚æ•° |
| 401 | é‰´æƒå¤±è´¥ | æ£€æŸ¥Token |
| 429 | è¯·æ±‚è¿‡å¿« | é™ä½è¯·æ±‚é¢‘ç‡ |
| 500 | æœåŠ¡å™¨é”™è¯¯ | è”ç³»æŠ€æœ¯æ”¯æŒ |

**ç¤ºä¾‹ä»£ç **

Python:
```python
import requests

response = requests.post(
    "https://api.example.com/v1/generate",
    headers={"Authorization": "Bearer YOUR_TOKEN"},
    json={
        "prompt": "ä»€ä¹ˆæ˜¯AIèŠ¯ç‰‡?",
        "max_tokens": 100
    }
)

result = response.json()
print(result["text"])
```

JavaScript:
```javascript
const response = await fetch('https://api.example.com/v1/generate', {
  method: 'POST',
  headers: {
    'Authorization': 'Bearer YOUR_TOKEN',
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    prompt: 'ä»€ä¹ˆæ˜¯AIèŠ¯ç‰‡?',
    max_tokens: 100
  })
});

const result = await response.json();
console.log(result.text);
```
```

---

**æµ‹è¯•æŠ¥å‘Šæ¨¡æ¿**

```markdown
# ChatGLM-6B INT8é‡åŒ–æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•ç¯å¢ƒ
- GPU: NVIDIA RTX 3090 (24GB)
- CPU: AMD Ryzen 9 5950X
- å†…å­˜: 64GB DDR4
- ç³»ç»Ÿ: Ubuntu 22.04
- CUDA: 11.8
- PyTorch: 2.1.0

## æµ‹è¯•æ–¹æ³•
ä½¿ç”¨æ ‡å‡†æµ‹è¯•é›†(100ä¸ªprompt,å¹³å‡é•¿åº¦50 tokens)è¿›è¡Œæ¨ç†,
é‡å¤3æ¬¡å–å¹³å‡å€¼ã€‚

## æµ‹è¯•ç»“æœ

### æ€§èƒ½å¯¹æ¯”

| é…ç½® | æ˜¾å­˜å ç”¨ | æ¨ç†é€Ÿåº¦ | TTFT | ååé‡ |
|------|----------|----------|------|--------|
| FP16 | 13.2 GB | 45 tok/s | 234ms | 8 req/s |
| INT8 | 7.1 GB â†“46% | 52 tok/s â†‘16% | 198ms â†“15% | 12 req/s â†‘50% |

### ç²¾åº¦è¯„ä¼°

ä½¿ç”¨MMLUåŸºå‡†æµ‹è¯•:
- FP16å‡†ç¡®ç‡: 61.2%
- INT8å‡†ç¡®ç‡: 60.8% (ä»…ä¸‹é™0.4%)

### å»¶è¿Ÿåˆ†å¸ƒ

![Latency Distribution](./images/latency_dist.png)

P50å»¶è¿Ÿ: 198ms
P95å»¶è¿Ÿ: 287ms
P99å»¶è¿Ÿ: 412ms

## ç»“è®º

âœ… INT8é‡åŒ–åœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶:
- æ˜¾å­˜å ç”¨å‡å°‘46%
- æ¨ç†é€Ÿåº¦æå‡16%
- ååé‡æå‡50%

âœ… æ¨èåœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨INT8é‡åŒ–

## é™„å½•

### å®Œæ•´æµ‹è¯•æ•°æ®
è§ `results/full_results.csv`

### å¤ç°æ­¥éª¤
```bash
python benchmark.py --config configs/int8.yaml
```
```

---

## ğŸ“ ç»¼åˆå­¦ä¹ è·¯çº¿

```infographic
infographic sequence-ascending-steps
data
  title 16å‘¨å®Œæ•´å­¦ä¹ è·¯å¾„
  items
    - label Week 1-4 åŸºç¡€
      desc Linux + Python + PyTorch
    - label Week 5-8 éƒ¨ç½²
      desc é‡åŒ– + ONNX + Docker
    - label Week 9-12 åº”ç”¨
      desc FastAPI + é£ä¹¦ + RAG
    - label Week 13-16 é¡¹ç›®
      desc 3ä¸ªå®Œæ•´Demo
```