# Demoå¼€å‘ä¸å®éªŒæŒ‡å— - AIèŠ¯ç‰‡åº”ç”¨å¼€å‘å²—ä½

```infographic
infographic list-grid-badge-card
data
  title ä¸‰å¤§Demoæ–¹å‘
  items
    - label å¤§æ¨¡å‹éƒ¨ç½²ä¼˜åŒ–
      desc å²—ä½æ ¸å¿ƒæŠ€èƒ½å±•ç¤º
      icon mdi:chip
    - label é£ä¹¦AIåº”ç”¨
      desc å²—ä½æ˜ç¡®è¦æ±‚
      icon mdi:robot
    - label Agent Sandbox
      desc åˆ›æ–°é¡¹ç›®äº®ç‚¹
      icon mdi:laboratory
```

---

## ğŸ“‹ Demo 1: å¤§æ¨¡å‹éƒ¨ç½²ä¼˜åŒ–

### é¡¹ç›®æ¦‚è¿°

**åç§°**ï¼šChatGLM-6Bé‡åŒ–éƒ¨ç½²ä¸æ€§èƒ½ä¼˜åŒ–

**ç›®æ ‡**ï¼š
- åœ¨æœ‰é™GPUèµ„æºä¸‹éƒ¨ç½²å¤§æ¨¡å‹
- åº”ç”¨å¤šç§ä¼˜åŒ–æŠ€æœ¯
- ç”Ÿæˆè¯¦ç»†æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

**æ—¶é—´å®‰æ’**ï¼šç¬¬8-10å‘¨ï¼ˆ3å‘¨ï¼‰

---

### æŠ€æœ¯è·¯çº¿

```infographic
infographic sequence-steps-simple
data
  title å¼€å‘æµç¨‹
  items
    - label ç¯å¢ƒå‡†å¤‡
      desc å®‰è£…ä¾èµ–ã€ä¸‹è½½æ¨¡å‹
    - label åŸºçº¿æµ‹è¯•
      desc FP16ç²¾åº¦æ€§èƒ½åŸºå‡†
    - label é‡åŒ–ä¼˜åŒ–
      desc INT8/INT4é‡åŒ–
    - label æ¨ç†åŠ é€Ÿ
      desc vLLM/Flash Attention
    - label æ€§èƒ½å¯¹æ¯”
      desc ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
```

---

### å®ç°æ­¥éª¤

#### æ­¥éª¤1: ç¯å¢ƒæ­å»ºï¼ˆDay 1-2ï¼‰

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir llm-deployment-demo && cd llm-deployment-demo

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n llm-deploy python=3.10
conda activate llm-deploy

# å®‰è£…ä¾èµ–
pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers==4.35.0
pip install accelerate==0.25.0
pip install bitsandbytes==0.41.0  # é‡åŒ–åº“
pip install vllm==0.2.6  # æ¨ç†åŠ é€Ÿ
pip install fastapi uvicorn  # APIæœåŠ¡
pip install locust  # å‹åŠ›æµ‹è¯•
```

**é¡¹ç›®ç»“æ„**ï¼š
```
llm-deployment-demo/
â”œâ”€â”€ models/               # æ¨¡å‹æƒé‡
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ inference.py     # æ¨ç†è„šæœ¬
â”‚   â”œâ”€â”€ quantize.py      # é‡åŒ–è„šæœ¬
â”‚   â”œâ”€â”€ benchmark.py     # æ€§èƒ½æµ‹è¯•
â”‚   â””â”€â”€ api_server.py    # FastAPIæœåŠ¡
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ model_config.yaml
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ results/         # æµ‹è¯•ç»“æœ
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb   # ç»“æœåˆ†æ
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

#### æ­¥éª¤2: æ¨¡å‹ä¸‹è½½ä¸åŸºçº¿æµ‹è¯•ï¼ˆDay 3-4ï¼‰

```python
# src/inference.py
from transformers import AutoTokenizer, AutoModel
import torch
import time

class BaselineInference:
    def __init__(self, model_name="THUDM/chatglm3-6b"):
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True
        )
        self.model = AutoModel.from_pretrained(
            model_name, 
            trust_remote_code=True,
            torch_dtype=torch.float16  # FP16åŸºçº¿
        ).cuda()
        self.model.eval()
    
    def generate(self, prompt, max_length=512):
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        
        start_time = time.time()
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                do_sample=True,
                temperature=0.7
            )
        latency = time.time() - start_time
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response, latency
    
    def get_memory_usage(self):
        """è·å–æ˜¾å­˜å ç”¨"""
        return torch.cuda.max_memory_allocated() / 1024**3  # GB

# æµ‹è¯•è„šæœ¬
if __name__ == "__main__":
    engine = BaselineInference()
    
    test_prompts = [
        "è§£é‡Šä»€ä¹ˆæ˜¯Transformer",
        "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åº",
        "AIèŠ¯ç‰‡çš„ä¸»è¦ç±»å‹æœ‰å“ªäº›ï¼Ÿ"
    ]
    
    print("=== FP16 Baseline ===")
    for prompt in test_prompts:
        response, latency = engine.generate(prompt)
        print(f"Prompt: {prompt}")
        print(f"Latency: {latency:.2f}s")
        print(f"Memory: {engine.get_memory_usage():.2f}GB\n")
```

---

#### æ­¥éª¤3: INT8é‡åŒ–ï¼ˆDay 5-7ï¼‰

```python
# src/quantize.py
from transformers import AutoTokenizer, AutoModel
import torch

class QuantizedInference:
    def __init__(self, model_name="THUDM/chatglm3-6b"):
        """ä½¿ç”¨bitsandbytesè¿›è¡ŒINT8é‡åŒ–"""
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True
        )
        
        # åŠ è½½INT8é‡åŒ–æ¨¡å‹
        self.model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            load_in_8bit=True,  # INT8é‡åŒ–
            device_map="auto"
        )
        self.model.eval()
    
    def generate(self, prompt, max_length=512):
        # åŒä¸Šï¼Œæ¨ç†ä»£ç 
        pass

# å¯¹æ¯”æµ‹è¯•
if __name__ == "__main__":
    print("Loading INT8 model...")
    int8_engine = QuantizedInference()
    
    # è¿è¡Œç›¸åŒæµ‹è¯•
    # ...
```

---

#### æ­¥éª¤4: vLLMæ¨ç†åŠ é€Ÿï¼ˆDay 8-10ï¼‰

```python
# src/vllm_inference.py
from vllm import LLM, SamplingParams

class VLLMInference:
    def __init__(self, model_name="THUDM/chatglm3-6b"):
        self.llm = LLM(
            model=model_name,
            tensor_parallel_size=1,  # å•GPU
            dtype="float16",
            max_model_len=2048
        )
        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=512
        )
    
    def generate_batch(self, prompts):
        """æ‰¹é‡æ¨ç†"""
        outputs = self.llm.generate(prompts, self.sampling_params)
        return [output.outputs[0].text for output in outputs]

# æ‰¹é‡æµ‹è¯•ï¼ˆä½“ç°ååé‡ä¼˜åŠ¿ï¼‰
if __name__ == "__main__":
    engine = VLLMInference()
    
    # æ‰¹é‡è¯·æ±‚
    batch_prompts = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"] * 10  # 30ä¸ªè¯·æ±‚
    
    start = time.time()
    results = engine.generate_batch(batch_prompts)
    total_time = time.time() - start
    
    print(f"Throughput: {len(batch_prompts) / total_time:.2f} req/s")
```

---

#### æ­¥éª¤5: APIæœåŠ¡ä¸å‹åŠ›æµ‹è¯•ï¼ˆDay 11-14ï¼‰

```python
# src/api_server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from vllm_inference import VLLMInference
import uvicorn

app = FastAPI(title="LLM Inference API")
engine = VLLMInference()

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 512

class GenerateResponse(BaseModel):
    text: str
    latency: float

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    import time
    start = time.time()
    
    result = engine.generate_batch([request.prompt])[0]
    latency = time.time() - start
    
    return GenerateResponse(text=result, latency=latency)

@app.get("/health")
async def health():
    return {"status": "ok"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**å‹åŠ›æµ‹è¯•**ï¼ˆLocustï¼‰ï¼š
```python
# benchmarks/locustfile.py
from locust import HttpUser, task, between

class LLMUser(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def generate_text(self):
        self.client.post("/generate", json={
            "prompt": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "max_tokens": 256
        })

# è¿è¡Œ: locust -f locustfile.py --host=http://localhost:8000
```

---

#### æ­¥éª¤6: æ€§èƒ½æµ‹è¯•ä¸æŠ¥å‘Šï¼ˆDay 15-21ï¼‰

**è‡ªåŠ¨åŒ–benchmarkè„šæœ¬**ï¼š
```python
# src/benchmark.py
import json
import matplotlib.pyplot as plt
import pandas as pd
from baseline_inference import BaselineInference
from quantized_inference import QuantizedInference
from vllm_inference import VLLMInference

def benchmark_all():
    engines = {
        "FP16 Baseline": BaselineInference(),
        "INT8 Quantized": QuantizedInference(),
        "vLLM FP16": VLLMInference()
    }
    
    test_prompts = [
        "çŸ­æç¤ºæµ‹è¯•",
        "ä¸­ç­‰é•¿åº¦çš„æç¤º" * 10,
        "å¾ˆé•¿çš„æç¤º" * 50
    ]
    
    results = []
    
    for name, engine in engines.items():
        print(f"Testing {name}...")
        for prompt_type, prompt in enumerate(test_prompts):
            latency, memory = engine.test(prompt)
            results.append({
                'Engine': name,
                'PromptType': f"Type{prompt_type+1}",
                'Latency(s)': latency,
                'Memory(GB)': memory
            })
    
    # ä¿å­˜ç»“æœ
    df = pd.DataFrame(results)
    df.to_csv('benchmarks/results/comparison.csv', index=False)
    
    # å¯è§†åŒ–
    plot_results(df)
    
    return df

def plot_results(df):
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # å»¶è¿Ÿå¯¹æ¯”
    df.pivot(index='PromptType', columns='Engine', values='Latency(s)').plot(
        kind='bar', ax=axes[0], title='Latency Comparison'
    )
    axes[0].set_ylabel('Latency (seconds)')
    
    # å†…å­˜å¯¹æ¯”
    df.groupby('Engine')['Memory(GB)'].mean().plot(
        kind='bar', ax=axes[1], title='Memory Usage'
    )
    axes[1].set_ylabel('Memory (GB)')
    
    plt.tight_layout()
    plt.savefig('benchmarks/results/comparison.png', dpi=300)
    print("Results saved to benchmarks/results/")

if __name__ == "__main__":
    benchmark_all()
```

**ç”ŸæˆæŠ€æœ¯æŠ¥å‘Š**ï¼š
```markdown
# ChatGLM-6B éƒ¨ç½²ä¼˜åŒ–æŠ¥å‘Š

## å®éªŒç¯å¢ƒ
- GPU: NVIDIA RTX 3090 (24GB)
- CUDA: 11.8
- PyTorch: 2.1.0

## ä¼˜åŒ–æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | å»¶è¿Ÿ (s) | æ˜¾å­˜ (GB) | ååé‡ (tok/s) | ç²¾åº¦æŸå¤± |
|------|----------|-----------|----------------|----------|
| FP16 Baseline | 2.34 | 13.2 | 45 | - |
| INT8 Quantized | 1.98 | 7.1 | 52 | <1% |
| vLLM+FP16 | 0.87 | 13.5 | 118 | - |
| vLLM+INT8 | 0.76 | 7.3 | 135 | <1% |

## å…³é”®å‘ç°
1. **vLLMå¸¦æ¥3xååé‡æå‡**ï¼ˆPagedAttentionï¼‰
2. **INT8é‡åŒ–èŠ‚çœ46%æ˜¾å­˜**ï¼Œæ€§èƒ½æŸå¤±å¯å¿½ç•¥
3. **ç»„åˆä¼˜åŒ–æ•ˆæœæœ€ä½³**ï¼švLLM+INT8

## ä¼˜åŒ–æŠ€æœ¯è¯¦è§£
### 1. INT8é‡åŒ–
- ä½¿ç”¨LLM.int8()ç®—æ³•
- æ··åˆç²¾åº¦ï¼šæ•æ„Ÿå±‚ä¿æŒFP16
- å®ç°ç»†èŠ‚ï¼š...

### 2. vLLMä¼˜åŒ–
- PagedAttentionå‡å°‘æ˜¾å­˜ç¢ç‰‡
- Continuous batchingæå‡åå
- ...

## ç»“è®º
é€šè¿‡é‡åŒ–å’Œæ¨ç†ä¼˜åŒ–ï¼Œåœ¨ä¿æŒç²¾åº¦çš„å‰æä¸‹ï¼š
- âœ… æ˜¾å­˜å ç”¨å‡å°‘46%
- âœ… æ¨ç†é€Ÿåº¦æå‡3å€
- âœ… å¯æ”¯æŒæ›´å¤§batch size
```

---

### Demoå±•ç¤ºæ–¹å¼

#### æ–¹å¼1: Jupyter Notebookäº¤äº’å¼å±•ç¤º

**åˆ›å»º**ï¼š`notebooks/demo.ipynb`

**å†…å®¹ç»“æ„**ï¼š
1. **é—®é¢˜å¼•å…¥**ï¼šå¤§æ¨¡å‹éƒ¨ç½²æŒ‘æˆ˜
2. **æ–¹æ¡ˆå¯¹æ¯”**ï¼šè¿è¡Œä¸åŒä¼˜åŒ–æ–¹æ³•
3. **å®æ—¶å¯è§†åŒ–**ï¼šæ˜¾å­˜å ç”¨ã€æ¨ç†é€Ÿåº¦
4. **ç»“è®ºæ€»ç»“**ï¼šæ€§èƒ½æå‡æ•°æ®

#### æ–¹å¼2: Gradio Webç•Œé¢

```python
# demo_app.py
import gradio as gr
from vllm_inference import VLLMInference

engine = VLLMInference()

def generate_text(prompt, method):
    """
    method: 'FP16', 'INT8', 'vLLM'
    """
    # æ ¹æ®methodé€‰æ‹©ä¸åŒå¼•æ“
    result = engine.generate([prompt])[0]
    return result

demo = gr.Interface(
    fn=generate_text,
    inputs=[
        gr.Textbox(label="è¾“å…¥æç¤º", placeholder="è¯·è¾“å…¥é—®é¢˜..."),
        gr.Dropdown(["FP16 Baseline", "INT8", "vLLM"], label="ä¼˜åŒ–æ–¹æ³•")
    ],
    outputs=gr.Textbox(label="æ¨¡å‹è¾“å‡º"),
    title="å¤§æ¨¡å‹éƒ¨ç½²ä¼˜åŒ–Demo",
    description="å¯¹æ¯”ä¸åŒä¼˜åŒ–æ–¹æ³•çš„æ•ˆæœ"
)

demo.launch()
```

#### æ–¹å¼3: å½•åˆ¶æ¼”ç¤ºè§†é¢‘ï¼ˆ5åˆ†é’Ÿï¼‰

**è„šæœ¬**ï¼š
1. **0:00-0:30** - ä»‹ç»èƒŒæ™¯ï¼ˆå¤§æ¨¡å‹éƒ¨ç½²æŒ‘æˆ˜ï¼‰
2. **0:30-1:30** - å±•ç¤ºä»£ç ç»“æ„ï¼ˆå¿«é€Ÿæµè§ˆï¼‰
3. **1:30-3:00** - è¿è¡Œbenchmarkï¼ˆå±å¹•å½•åˆ¶ï¼‰
4. **3:00-4:00** - ç»“æœå¯è§†åŒ–ï¼ˆå›¾è¡¨è®²è§£ï¼‰
5. **4:00-5:00** - æ€»ç»“ä¸æŠ€æœ¯è¦ç‚¹

**å·¥å…·**ï¼šOBS Studioå½•å±

---

## ğŸ“± Demo 2: é£ä¹¦AIæ™ºèƒ½åŠ©æ‰‹

### é¡¹ç›®æ¦‚è¿°

**åç§°**ï¼šé£ä¹¦çŸ¥è¯†åº“é—®ç­”æœºå™¨äºº

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
1. æ¥æ”¶é£ä¹¦æ¶ˆæ¯
2. æ£€ç´¢çŸ¥è¯†åº“ï¼ˆRAGï¼‰
3. è°ƒç”¨æœ¬åœ°LLMç”Ÿæˆå›ç­”
4. è¿”å›é£ä¹¦

**æ—¶é—´å®‰æ’**ï¼šç¬¬13-15å‘¨ï¼ˆ3å‘¨ï¼‰

---

### ç³»ç»Ÿæ¶æ„

```infographic
infographic hierarchy-tree-curved-line-rounded-rect-node
data
  title é£ä¹¦AIåŠ©æ‰‹æ¶æ„
  items
    - label é£ä¹¦å®¢æˆ·ç«¯
      children:
        - label ç”¨æˆ·å‘é€æ¶ˆæ¯
    - label é£ä¹¦å¼€æ”¾å¹³å°
      children:
        - label Webhookå›è°ƒ
        - label æ¶ˆæ¯API
    - label åç«¯æœåŠ¡ (FastAPI)
      children:
        - label RAGæ£€ç´¢æ¨¡å—
        - label LLMæ¨ç†æ¨¡å—
        - label å¯¹è¯ç®¡ç†
    - label æ•°æ®å±‚
      children:
        - label å‘é‡æ•°æ®åº“
        - label å¯¹è¯å†å²
```

---

### å®ç°æ­¥éª¤

#### æ­¥éª¤1: é£ä¹¦åº”ç”¨åˆ›å»ºï¼ˆDay 1-2ï¼‰

**æ“ä½œæµç¨‹**ï¼š
1. è®¿é—® https://open.feishu.cn/
2. åˆ›å»ºä¼ä¸šè‡ªå»ºåº”ç”¨
3. è·å– App ID å’Œ App Secret
4. é…ç½®æƒé™ï¼š
   - è¯»å–æ¶ˆæ¯
   - å‘é€æ¶ˆæ¯
   - è·å–ç”¨æˆ·ä¿¡æ¯
5. é…ç½®äº‹ä»¶è®¢é˜…URLï¼ˆåç»­å¡«å†™ï¼‰

**é…ç½®æ–‡ä»¶**ï¼š
```yaml
# config/feishu_config.yaml
app_id: "cli_xxxxx"
app_secret: "xxxxxx"
verification_token: "xxxxxx"
encrypt_key: "xxxxxx"  # å¯é€‰

webhook_url: "https://your-server.com/feishu/webhook"
```

---

#### æ­¥éª¤2: åç«¯æœåŠ¡æ­å»ºï¼ˆDay 3-7ï¼‰

```python
# src/feishu_bot.py
from fastapi import FastAPI, Request, HTTPException
from lark_oapi.api.im.v1 import *
import lark_oapi as lark
import os

app = FastAPI()

# åˆå§‹åŒ–é£ä¹¦å®¢æˆ·ç«¯
client = lark.Client.builder() \
    .app_id(os.getenv("FEISHU_APP_ID")) \
    .app_secret(os.getenv("FEISHU_APP_SECRET")) \
    .build()

@app.post("/feishu/webhook")
async def feishu_webhook(request: Request):
    """æ¥æ”¶é£ä¹¦äº‹ä»¶å›è°ƒ"""
    body = await request.json()
    
    # éªŒè¯challenge
    if "challenge" in body:
        return {"challenge": body["challenge"]}
    
    # å¤„ç†æ¶ˆæ¯äº‹ä»¶
    if body.get("header", {}).get("event_type") == "im.message.receive_v1":
        await handle_message(body)
    
    return {"code": 0}

async def handle_message(event_data):
    """å¤„ç†æ¥æ”¶åˆ°çš„æ¶ˆæ¯"""
    message = event_data["event"]["message"]
    content = json.loads(message["content"])
    user_input = content.get("text", "")
    
    # è°ƒç”¨AIç”Ÿæˆå›å¤
    ai_response = await generate_response(user_input)
    
    # å‘é€å›å¤åˆ°é£ä¹¦
    await send_message(message["chat_id"], ai_response)

async def send_message(chat_id, text):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦"""
    request = CreateMessageRequest.builder() \
        .receive_id_type("chat_id") \
        .request_body(
            CreateMessageRequestBody.builder()
            .receive_id(chat_id)
            .msg_type("text")
            .content(json.dumps({"text": text}))
            .build()
        ).build()
    
    response = client.im.v1.message.create(request)
    
    if not response.success():
        print(f"Error: {response.msg}")

# AIç”Ÿæˆé€»è¾‘ï¼ˆä¸‹ä¸€æ­¥å®ç°ï¼‰
async def generate_response(user_input):
    # TODO: é›†æˆLLM
    return "æ”¶åˆ°ï¼š" + user_input
```

---

#### æ­¥éª¤3: RAGçŸ¥è¯†åº“é›†æˆï¼ˆDay 8-12ï¼‰

```python
# src/rag_engine.py
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader

class RAGEngine:
    def __init__(self, knowledge_base_path="./knowledge_base"):
        # åŠ è½½æ–‡æ¡£
        loader = DirectoryLoader(knowledge_base_path, glob="**/*.md")
        documents = loader.load()
        
        # åˆ†å‰²æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        texts = text_splitter.split_documents(documents)
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-small-zh-v1.5"  # ä¸­æ–‡embedding
        )
        self.vectorstore = FAISS.from_documents(texts, embeddings)
    
    def retrieve(self, query, top_k=3):
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        docs = self.vectorstore.similarity_search(query, k=top_k)
        context = "\n\n".join([doc.page_content for doc in docs])
        return context

# é›†æˆåˆ°æ¶ˆæ¯å¤„ç†
rag_engine = RAGEngine()

async def generate_response(user_input):
    # æ£€ç´¢ç›¸å…³çŸ¥è¯†
    context = rag_engine.retrieve(user_input)
    
    # æ„å»ºprompt
    prompt = f"""æ ¹æ®ä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ï¼š

çŸ¥è¯†åº“ï¼š
{context}

é—®é¢˜ï¼š{user_input}

å›ç­”ï¼š"""
    
    # è°ƒç”¨LLMï¼ˆä½¿ç”¨Demo1ä¸­çš„æ¨ç†å¼•æ“ï¼‰
    from vllm_inference import VLLMInference
    llm = VLLMInference()
    response = llm.generate_batch([prompt])[0]
    
    return response
```

---

#### æ­¥éª¤4: å¯¹è¯å†å²ç®¡ç†ï¼ˆDay 13-15ï¼‰

```python
# src/conversation_manager.py
from collections import defaultdict
import json

class ConversationManager:
    def __init__(self, max_history=5):
        self.conversations = defaultdict(list)
        self.max_history = max_history
    
    def add_message(self, chat_id, role, content):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        self.conversations[chat_id].append({
            "role": role,
            "content": content
        })
        
        # ä¿æŒæœ€è¿‘Nè½®å¯¹è¯
        if len(self.conversations[chat_id]) > self.max_history * 2:
            self.conversations[chat_id] = self.conversations[chat_id][-self.max_history*2:]
    
    def get_history(self, chat_id):
        """è·å–å¯¹è¯å†å²"""
        return self.conversations[chat_id]
    
    def format_prompt(self, chat_id, current_query, context=""):
        """æ ¼å¼åŒ–ä¸ºæ¨¡å‹è¾“å…¥"""
        history = self.get_history(chat_id)
        
        prompt = f"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚\n\n"
        
        if context:
            prompt += f"å‚è€ƒä¿¡æ¯ï¼š\n{context}\n\n"
        
        prompt += "å¯¹è¯å†å²ï¼š\n"
        for msg in history:
            prompt += f"{msg['role']}: {msg['content']}\n"
        
        prompt += f"ç”¨æˆ·: {current_query}\nåŠ©æ‰‹: "
        
        return prompt

# æ›´æ–°generate_response
conv_manager = ConversationManager()

async def generate_response(user_input, chat_id):
    # æ£€ç´¢çŸ¥è¯†
    context = rag_engine.retrieve(user_input)
    
    # æ„å»ºå¸¦å†å²çš„prompt
    prompt = conv_manager.format_prompt(chat_id, user_input, context)
    
    # ç”Ÿæˆå›å¤
    response = llm.generate_batch([prompt])[0]
    
    # ä¿å­˜å¯¹è¯
    conv_manager.add_message(chat_id, "ç”¨æˆ·", user_input)
    conv_manager.add_message(chat_id, "åŠ©æ‰‹", response)
    
    return response
```

---

#### æ­¥éª¤5: éƒ¨ç½²ä¸æµ‹è¯•ï¼ˆDay 16-21ï¼‰

**Dockeréƒ¨ç½²**ï¼š
```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
COPY config/ ./config/
COPY knowledge_base/ ./knowledge_base/

EXPOSE 8000

CMD ["uvicorn", "src.feishu_bot:app", "--host", "0.0.0.0", "--port", "8000"]
```

**å†…ç½‘ç©¿é€æµ‹è¯•**ï¼ˆå¼€å‘é˜¶æ®µï¼‰ï¼š
```bash
# ä½¿ç”¨ngrokæš´éœ²æœ¬åœ°æœåŠ¡
ngrok http 8000

# å°†ç”Ÿæˆçš„URLé…ç½®åˆ°é£ä¹¦åº”ç”¨çš„äº‹ä»¶è®¢é˜…åœ°å€
```

---

### Demoå±•ç¤º

#### å±•ç¤ºè„šæœ¬

1. **é—®é¢˜æ¼”ç¤º**ï¼šåœ¨é£ä¹¦ä¸­å‘é€é—®é¢˜
   - "å…¬å¸çš„AIèŠ¯ç‰‡æ”¯æŒå“ªäº›æ¡†æ¶ï¼Ÿ"
   - "å¦‚ä½•éƒ¨ç½²å¤§æ¨¡å‹ï¼Ÿ"

2. **åå°å±•ç¤º**ï¼š
   - ç»ˆç«¯æ˜¾ç¤ºæ¥æ”¶åˆ°æ¶ˆæ¯
   - RAGæ£€ç´¢æ—¥å¿—
   - LLMç”Ÿæˆè¿‡ç¨‹

3. **ç»“æœå±•ç¤º**ï¼šé£ä¹¦ä¸­æ”¶åˆ°AIå›å¤

4. **æŠ€æœ¯è®²è§£**ï¼š
   - RAGæ£€ç´¢æœºåˆ¶
   - å¯¹è¯å†å²ç®¡ç†
   - é£ä¹¦APIé›†æˆ

---

## ğŸ¤– Demo 3: Agent Sandbox åŸå‹

### å¿«é€ŸåŸå‹ï¼ˆMVPï¼‰

**æ—¶é—´**ï¼šç¬¬16å‘¨ï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**ï¼šè¯æ˜æ¦‚å¿µå¯è¡Œæ€§

```python
# sandbox_mvp.py
import gymnasium as gym
import torch
from stable_baselines3 import PPO
import time

class ChipAwarePPO:
    """ç®€åŒ–ç‰ˆèŠ¯ç‰‡æ„ŸçŸ¥è®­ç»ƒ"""
    
    def __init__(self, env_name="CartPole-v1", device="cuda"):
        self.env = gym.make(env_name)
        self.device = device
        
        self.model = PPO(
            "MlpPolicy",
            self.env,
            device=device,
            verbose=1,
            tensorboard_log="./logs/"
        )
    
    def train_with_monitoring(self, total_timesteps=10000):
        """è®­ç»ƒå¹¶ç›‘æ§ç¡¬ä»¶"""
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        
        callback = GPUMonitorCallback(handle)
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback
        )
    
    def evaluate(self):
        """è¯„ä¼°Agent"""
        obs, _ = self.env.reset()
        total_reward = 0
        
        for _ in range(500):
            action, _ = self.model.predict(obs)
            obs, reward, done, truncated, _ = self.env.step(action)
            total_reward += reward
            
            if done or truncated:
                break
        
        return total_reward

from stable_baselines3.common.callbacks import BaseCallback

class GPUMonitorCallback(BaseCallback):
    def __init__(self, gpu_handle):
        super().__init__()
        self.gpu_handle = gpu_handle
    
    def _on_step(self):
        if self.n_calls % 100 == 0:
            util = pynvml.nvmlDeviceGetUtilizationRates(self.gpu_handle)
            print(f"Step {self.n_calls}: GPU Util = {util.gpu}%")
        return True

# è¿è¡ŒDemo
if __name__ == "__main__":
    agent = ChipAwarePPO()
    
    print("å¼€å§‹è®­ç»ƒ...")
    agent.train_with_monitoring(total_timesteps=50000)
    
    print("è¯„ä¼°Agent...")
    reward = agent.evaluate()
    print(f"æ€»å¥–åŠ±: {reward}")
```

**å±•ç¤ºè¦ç‚¹**ï¼š
- å®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹
- TensorBoardå¯è§†åŒ–
- GPUåˆ©ç”¨ç‡ç›‘æ§
- å¯¹æ¯”ä¸åŒè®¾å¤‡ï¼ˆCPU vs GPUï¼‰


### 1. å¤§æ¨¡å‹éƒ¨ç½²ä¼˜åŒ–
- å®ç°ChatGLM-6Bçš„INT8é‡åŒ–ï¼Œæ˜¾å­˜å ç”¨å‡å°‘46%
- ä½¿ç”¨vLLMä¼˜åŒ–ï¼Œæ¨ç†é€Ÿåº¦æå‡3å€
- [æŸ¥çœ‹è¯¦æƒ…](./projects/llm-deployment/) | [GitHub](https://github.com/ä½ çš„ç”¨æˆ·å/llm-deployment)

### 2. é£ä¹¦AIæ™ºèƒ½åŠ©æ‰‹
- åŸºäºRAGçš„çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
- æ”¯æŒä¸Šä¸‹æ–‡å¯¹è¯
- [åœ¨çº¿Demo](é“¾æ¥) | [GitHub](...)

### 3. Agent SandboxåŸå‹
- ç¡¬ä»¶æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
- æ”¯æŒNVIDIA/AMDå¤šèŠ¯ç‰‡é€‚é…
- [æŠ€æœ¯æ–‡æ¡£](é“¾æ¥) | [GitHub](...)

## æŠ€èƒ½çŸ©é˜µ
- Python, C++, CUDA
- PyTorch, Transformers, vLLM
- é£ä¹¦å¼€æ”¾å¹³å°å¼€å‘
- æ¨¡å‹é‡åŒ–ä¸ä¼˜åŒ–

## ğŸ§ª å®éªŒè®°å½•è§„èŒƒ

### å®éªŒæ—¥å¿—æ¨¡æ¿

```markdown
# å®éªŒæ—¥å¿— - [æ—¥æœŸ]

## å®éªŒç›®æ ‡
æ˜ç¡®æœ¬æ¬¡å®éªŒè¦éªŒè¯ä»€ä¹ˆ

## å®éªŒé…ç½®
- ç¡¬ä»¶ï¼šGPUå‹å·ã€å†…å­˜
- è½¯ä»¶ï¼šæ¡†æ¶ç‰ˆæœ¬
- æ¨¡å‹ï¼šæ¨¡å‹åç§°ã€å‚æ•°é‡

## å®éªŒæ­¥éª¤
1. ...
2. ...

## å®éªŒç»“æœ
### å®šé‡ç»“æœ
| æŒ‡æ ‡ | å€¼ |
|------|---|
| ... | ... |

### å®šæ€§è§‚å¯Ÿ
- ç°è±¡1
- ç°è±¡2

## é—®é¢˜ä¸è§£å†³
- **é—®é¢˜**ï¼šCUDA out of memory
  - **è§£å†³**ï¼šå‡å°batch sizeåˆ°16

---

## âœ… æœ€ç»ˆæ£€æŸ¥æ¸…å•

### Demo 1: å¤§æ¨¡å‹éƒ¨ç½²
- [ ] ä»£ç è¿è¡Œæ— è¯¯
- [ ] ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
- [ ] å¯è§†åŒ–å›¾è¡¨æ¸…æ™°
- [ ] READMEæ–‡æ¡£å®Œæ•´
- [ ] å½•åˆ¶æ¼”ç¤ºè§†é¢‘

### Demo 2: é£ä¹¦AIåŠ©æ‰‹
- [ ] é£ä¹¦åº”ç”¨é…ç½®æ­£ç¡®
- [ ] RAGæ£€ç´¢åŠŸèƒ½æ­£å¸¸
- [ ] å¯¹è¯å†å²ç®¡ç†ç”Ÿæ•ˆ
- [ ] éƒ¨ç½²æ–‡æ¡£è¯¦ç»†
- [ ] å‡†å¤‡æµ‹è¯•å¯¹è¯æ¡ˆä¾‹

### Demo 3: Agent Sandbox
- [ ] MVPåŸå‹å¯è¿è¡Œ
- [ ] GPUç›‘æ§åŠŸèƒ½æ­£å¸¸
- [ ] TensorBoardå¯è§†åŒ–
- [ ] æ¶æ„æ–‡æ¡£æ’°å†™
- [ ] æœªæ¥è§„åˆ’æ¸…æ™°
---
## ğŸ“š èµ„æºé“¾æ¥æ±‡æ€»
### å®˜æ–¹æ–‡æ¡£
- é£ä¹¦å¼€æ”¾å¹³å°: https://open.feishu.cn/document/
- vLLMæ–‡æ¡£: https://docs.vllm.ai/
- Stable-Baselines3: https://stable-baselines3.readthedocs.io/

