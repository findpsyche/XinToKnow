# AIèŠ¯ç‰‡ä¸å¤§æ¨¡å‹éƒ¨ç½² - é¡¶çº§è®ºæ–‡ä¸ç ”ç©¶èµ„æº

## ğŸ“š ç ”ç©¶é¢†åŸŸæ¦‚è§ˆ

æœ¬æ–‡æ¡£æ•´ç†äº†AIèŠ¯ç‰‡ã€æ¨¡å‹ä¼˜åŒ–ã€å¤§æ¨¡å‹éƒ¨ç½²ä¸‰å¤§æ ¸å¿ƒé¢†åŸŸçš„é¡¶çº§ç ”ç©¶èµ„æºã€‚

---

## ğŸ›ï¸ é¡¶çº§ç ”ç©¶å®éªŒå®¤ä¸æœºæ„

### å›½é™…é¡¶çº§å®éªŒå®¤

#### 1. **Stanford University**
- **HAI (Human-Centered AI Institute)**
  - ç ”ç©¶æ–¹å‘ï¼šå¤§æ¨¡å‹ã€AIç³»ç»Ÿ
  - ç½‘ç«™ï¼šhttps://hai.stanford.edu/
  
- **DAWN Lab (Data Analytics on What's Next)**
  - ç ”ç©¶æ–¹å‘ï¼šMLç³»ç»Ÿã€æ¨¡å‹æœåŠ¡
  - ç½‘ç«™ï¼šhttps://dawn.cs.stanford.edu/
  - ä»£è¡¨é¡¹ç›®ï¼šHazy Research (Flash Attentionå›¢é˜Ÿ)

- **Platform Lab**
  - ç ”ç©¶æ–¹å‘ï¼šç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–
  - ç½‘ç«™ï¼šhttps://platformlab.stanford.edu/

#### 2. **UC Berkeley**
- **Sky Computing Lab (å‰ RISELab)**
  - ç ”ç©¶æ–¹å‘ï¼šåˆ†å¸ƒå¼MLç³»ç»Ÿ
  - ç½‘ç«™ï¼šhttps://sky.cs.berkeley.edu/
  - ä»£è¡¨é¡¹ç›®ï¼šRay, Modin
  
- **BAIR (Berkeley AI Research)**
  - ç ”ç©¶æ–¹å‘ï¼šAIç®—æ³•ä¸ç³»ç»Ÿ
  - ç½‘ç«™ï¼šhttps://bair.berkeley.edu/

#### 3. **MIT**
- **MIT CSAIL**
  - ç ”ç©¶æ–¹å‘ï¼šAIç³»ç»Ÿã€ç¼–è¯‘å™¨ä¼˜åŒ–
  - ç½‘ç«™ï¼šhttps://www.csail.mit.edu/
  - ä»£è¡¨é¡¹ç›®ï¼šTVM (æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨)

- **Han Lab (Song Han)**
  - ç ”ç©¶æ–¹å‘ï¼šé«˜æ•ˆæ·±åº¦å­¦ä¹ ã€æ¨¡å‹å‹ç¼©
  - ç½‘ç«™ï¼šhttps://hanlab.mit.edu/
  - ä»£è¡¨å·¥ä½œï¼šTinyML, MCUNet, AWQ

#### 4. **Carnegie Mellon University (CMU)**
- **Catalyst Group**
  - ç ”ç©¶æ–¹å‘ï¼šMLç³»ç»Ÿã€GPUä¼˜åŒ–
  - ä»£è¡¨é¡¹ç›®ï¼šFlexFlow

- **PDL (Parallel Data Lab)**
  - ç ”ç©¶æ–¹å‘ï¼šå­˜å‚¨ä¸ç³»ç»Ÿä¼˜åŒ–

#### 5. **University of Washington**
- **SAMPL (Systems and Machine Learning)**
  - ç ”ç©¶æ–¹å‘ï¼šMLç¼–è¯‘ã€ç³»ç»Ÿä¼˜åŒ–
  - ä»£è¡¨é¡¹ç›®ï¼šTVM, Apache MXNet

#### 6. **Princeton University**
- **Princeton NLP Group**
  - ç ”ç©¶æ–¹å‘ï¼šé«˜æ•ˆTransformer
  - ä»£è¡¨å·¥ä½œï¼šEfficient Transformers Survey

---

### å·¥ä¸šç ”ç©¶é™¢

#### AIèŠ¯ç‰‡å…¬å¸ç ”ç©¶éƒ¨é—¨

**NVIDIA Research**
- ç½‘ç«™ï¼šhttps://www.nvidia.com/en-us/research/
- æ ¸å¿ƒæ–¹å‘ï¼šGPUæ¶æ„ã€Transformerä¼˜åŒ–ã€æ¨ç†åŠ é€Ÿ
- ä»£è¡¨è®ºæ–‡ï¼šFlash Attentionåˆä½œã€Megatron-LM

**Google Research / DeepMind**
- ç½‘ç«™ï¼šhttps://research.google/ ã€ https://www.deepmind.com/research
- æ ¸å¿ƒæ–¹å‘ï¼šTPUä¼˜åŒ–ã€å¤§æ¨¡å‹æ¶æ„
- ä»£è¡¨è®ºæ–‡ï¼šTransformeråŸè®ºæ–‡ã€PaLMã€Gemini

**Meta AI (FAIR)**
- ç½‘ç«™ï¼šhttps://ai.meta.com/research/
- æ ¸å¿ƒæ–¹å‘ï¼šLLaMAç³»åˆ—ã€æ¨ç†ä¼˜åŒ–
- ä»£è¡¨é¡¹ç›®ï¼šPyTorchã€LLaMA

**Microsoft Research**
- ç½‘ç«™ï¼šhttps://www.microsoft.com/en-us/research/
- æ ¸å¿ƒæ–¹å‘ï¼šDeepSpeedã€ONNX Runtime
- ä»£è¡¨é¡¹ç›®ï¼šDeepSpeed, Megatron-DeepSpeed

**OpenAI**
- ç½‘ç«™ï¼šhttps://openai.com/research
- æ ¸å¿ƒæ–¹å‘ï¼šGPTç³»åˆ—ã€å¯¹é½æŠ€æœ¯
- ä»£è¡¨è®ºæ–‡ï¼šGPT-3/4ã€RLHF

**Anthropic**
- æ ¸å¿ƒæ–¹å‘ï¼šClaudeæ¨¡å‹ã€Constitutional AI
- ç ”ç©¶é‡ç‚¹ï¼šå®‰å…¨å¯¹é½ã€é«˜æ•ˆæ¨ç†

#### ä¸­å›½AIèŠ¯ç‰‡å…¬å¸

**åä¸ºæµ·æ€ (HiSilicon)**
- äº§å“ï¼šæ˜‡è…¾ç³»åˆ—AIèŠ¯ç‰‡
- å¼€æºé¡¹ç›®ï¼šMindSpore

**å¯’æ­¦çºª (Cambricon)**
- äº§å“ï¼šæ€å…ƒç³»åˆ—
- ç ”ç©¶æ–¹å‘ï¼šç¥ç»ç½‘ç»œå¤„ç†å™¨æ¶æ„

**åœ°å¹³çº¿ (Horizon Robotics)**
- äº§å“ï¼šå¾ç¨‹ç³»åˆ—ï¼ˆè¾¹ç¼˜AIèŠ¯ç‰‡ï¼‰
- åº”ç”¨åœºæ™¯ï¼šè‡ªåŠ¨é©¾é©¶

**å£ä»ç§‘æŠ€ (Biren Technology)**
- äº§å“ï¼šBR100ç³»åˆ—é€šç”¨GPU

**æ›¦æœ›sunrise**ï¼ˆç›®æ ‡å…¬å¸ï¼‰
- è‡ªç ”AIèŠ¯ç‰‡
- ä¸é£ä¹¦ç”Ÿæ€é›†æˆ
- **å»ºè®®é‡ç‚¹å…³æ³¨å…¶å®˜æ–¹æŠ€æœ¯åšå®¢å’Œå¼€æºé¡¹ç›®**

---

## ğŸ“„ æ ¸å¿ƒé¢†åŸŸå¿…è¯»è®ºæ–‡

### ä¸€ã€TransformeråŸºç¡€æ¶æ„

#### é‡Œç¨‹ç¢‘è®ºæ–‡

1. **Attention Is All You Need** (2017)
   - ä½œè€…ï¼šVaswani et al. (Google)
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/1706.03762
   - é‡è¦æ€§ï¼šâ­â­â­â­â­ (å¿…è¯»)
   - è´¡çŒ®ï¼šæå‡ºTransformeræ¶æ„

2. **BERT: Pre-training of Deep Bidirectional Transformers** (2018)
   - ä½œè€…ï¼šDevlin et al. (Google)
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/1810.04805
   - é‡è¦æ€§ï¼šâ­â­â­â­â­
   - è´¡çŒ®ï¼šåŒå‘é¢„è®­ç»ƒ

3. **Language Models are Few-Shot Learners (GPT-3)** (2020)
   - ä½œè€…ï¼šBrown et al. (OpenAI)
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2005.14165
   - é‡è¦æ€§ï¼šâ­â­â­â­â­
   - è´¡çŒ®ï¼šå±•ç¤ºå¤§æ¨¡å‹æ¶Œç°èƒ½åŠ›

---

### äºŒã€é«˜æ•ˆTransformeræ¶æ„

#### Attentionä¼˜åŒ–

4. **Flash Attention: Fast and Memory-Efficient Exact Attention** (2022)
   - ä½œè€…ï¼šDao et al. (Stanford)
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2205.14135
   - é‡è¦æ€§ï¼šâ­â­â­â­â­ (å¿…è¯»)
   - è´¡çŒ®ï¼šIOæ„ŸçŸ¥çš„æ³¨æ„åŠ›ç®—æ³•
   - ä»£ç ï¼šhttps://github.com/Dao-AILab/flash-attention

5. **Flash Attention-2** (2023)
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2307.08691
   - é‡è¦æ€§ï¼šâ­â­â­â­â­
   - è´¡çŒ®ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œ2-3xåŠ é€Ÿ

6. **Self-Attention Does Not Need O(nÂ²) Memory** (2021)
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2112.05682
   - è´¡çŒ®ï¼šå†…å­˜é«˜æ•ˆçš„attentionå®ç°

#### é•¿åºåˆ—å¤„ç†

7. **LongFormer: The Long-Document Transformer** (2020)
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2004.05150
   - è´¡çŒ®ï¼šç¨€ç–æ³¨æ„åŠ›

8. **Transformer-XL** (2019)
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/1901.02860
   - è´¡çŒ®ï¼šæ®µçº§å¾ªç¯æœºåˆ¶

---

### ä¸‰ã€æ¨¡å‹å‹ç¼©ä¸é‡åŒ–

#### é‡åŒ–æŠ€æœ¯

9. **LLM.int8(): 8-bit Matrix Multiplication for Transformers** (2022)
   - ä½œè€…ï¼šDettmers et al.
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2208.07339
   - é‡è¦æ€§ï¼šâ­â­â­â­â­ (å¿…è¯»)
   - è´¡çŒ®ï¼šæ— ç²¾åº¦æŸå¤±çš„8-bitæ¨ç†
   - ä»£ç ï¼šhttps://github.com/TimDettmers/bitsandbytes

10. **GPTQ: Accurate Post-Training Quantization for GPT** (2022)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2210.17323
    - é‡è¦æ€§ï¼šâ­â­â­â­â­
    - è´¡çŒ®ï¼šé«˜æ•ˆçš„æƒé‡é‡åŒ–
    - ä»£ç ï¼šhttps://github.com/IST-DASLab/gptq

11. **AWQ: Activation-aware Weight Quantization** (2023)
    - ä½œè€…ï¼šLin et al. (MIT Han Lab)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2306.00978
    - é‡è¦æ€§ï¼šâ­â­â­â­â­
    - è´¡çŒ®ï¼šæ¿€æ´»æ„ŸçŸ¥é‡åŒ–ï¼Œsotaæ€§èƒ½
    - ä»£ç ï¼šhttps://github.com/mit-han-lab/llm-awq

12. **SmoothQuant** (2022)
    - ä½œè€…ï¼šXiao et al. (MIT)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2211.10438
    - è´¡çŒ®ï¼šå¹³æ»‘æ¿€æ´»åˆ†å¸ƒå®ç°é‡åŒ–

#### å‰ªæä¸è’¸é¦

13. **The Lottery Ticket Hypothesis** (2019)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/1803.03635
    - è´¡çŒ®ï¼šç¨€ç–å­ç½‘ç»œå‘ç°

14. **DistilBERT** (2019)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/1910.01108
    - è´¡çŒ®ï¼šçŸ¥è¯†è’¸é¦åº”ç”¨äºBERT

---

### å››ã€å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–

#### æ¨ç†ç³»ç»Ÿ

15. **Orca: A Distributed Serving System for Transformer-Based Models** (2022)
    - é“¾æ¥ï¼šhttps://www.usenix.org/conference/osdi22/presentation/yu
    - è´¡çŒ®ï¼šåˆ†å¸ƒå¼æ¨ç†è°ƒåº¦

16. **Efficient Memory Management for LLM Serving with PagedAttention (vLLM)** (2023)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2309.06180
    - é‡è¦æ€§ï¼šâ­â­â­â­â­ (å¿…è¯»)
    - è´¡çŒ®ï¼šåˆ†é¡µæ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡ååé‡
    - ä»£ç ï¼šhttps://github.com/vllm-project/vllm

17. **FlexGen: High-Throughput Generative Inference** (2023)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2303.06865
    - è´¡çŒ®ï¼šæœ‰é™GPUæ¨ç†å¤§æ¨¡å‹

#### åŠ é€ŸæŠ€æœ¯

18. **Speculative Decoding** (2023)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2211.17192
    - é‡è¦æ€§ï¼šâ­â­â­â­
    - è´¡çŒ®ï¼šæ¨æµ‹æ€§è§£ç åŠ é€Ÿç”Ÿæˆ

19. **Medusa: Simple LLM Inference Acceleration with Multiple Decoding Heads** (2024)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.10774
    - è´¡çŒ®ï¼šå¤šå¤´å¹¶è¡Œè§£ç 

---

### äº”ã€å¼€æºå¤§æ¨¡å‹

20. **LLaMA: Open and Efficient Foundation Language Models** (2023)
    - ä½œè€…ï¼šTouvron et al. (Meta)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.13971
    - é‡è¦æ€§ï¼šâ­â­â­â­â­
    - è´¡çŒ®ï¼šå¼€æºé«˜è´¨é‡åŸºåº§æ¨¡å‹

21. **LLaMA 2: Open Foundation and Fine-Tuned Chat Models** (2023)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2307.09288
    - é‡è¦æ€§ï¼šâ­â­â­â­â­

22. **Mistral 7B** (2023)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2310.06825
    - è´¡çŒ®ï¼šsotaå°å‚æ•°é‡æ¨¡å‹

23. **Qwen Technical Report** (2023)
    - é˜¿é‡Œäº‘é€šä¹‰åƒé—®
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2309.16609

---

### å…­ã€AIèŠ¯ç‰‡æ¶æ„

24. **In-Datacenter Performance Analysis of a Tensor Processing Unit (TPU)** (2017)
    - ä½œè€…ï¼šJouppi et al. (Google)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/1704.04760
    - é‡è¦æ€§ï¼šâ­â­â­â­â­
    - è´¡çŒ®ï¼šTPUæ¶æ„è¯¦è§£

25. **NVIDIA A100 Tensor Core GPU Architecture** (White Paper)
    - é“¾æ¥ï¼šhttps://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf

26. **Understanding Latency Hiding on GPUs** (2016)
    - é“¾æ¥ï¼šhttps://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.pdf

---

### ä¸ƒã€ç¼–è¯‘ä¼˜åŒ–

27. **TVM: An Automated End-to-End Optimizing Compiler** (2018)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/1802.04799
    - é‡è¦æ€§ï¼šâ­â­â­â­
    - è´¡çŒ®ï¼šæ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨
    - ä»£ç ï¼šhttps://github.com/apache/tvm

28. **Ansor: Generating High-Performance Tensor Programs** (2020)
    - é“¾æ¥ï¼šhttps://arxiv.org/abs/2006.06762
    - è´¡çŒ®ï¼šè‡ªåŠ¨ç®—å­ä¼˜åŒ–

---

## ğŸ” è®ºæ–‡æŸ¥æ‰¾èµ„æº

### å­¦æœ¯æœç´¢å¼•æ“

1. **arXiv.org**
   - é“¾æ¥ï¼šhttps://arxiv.org/
   - åˆ†ç±»ï¼šcs.AI, cs.LG, cs.DC (åˆ†å¸ƒå¼è®¡ç®—), cs.AR (æ¶æ„)
   - ä½¿ç”¨æŠ€å·§ï¼šè®¢é˜…æ¯æ—¥arXivé‚®ä»¶æ‘˜è¦

2. **Google Scholar**
   - é“¾æ¥ï¼šhttps://scholar.google.com/
   - åˆ›å»ºalertsè·Ÿè¸ªå…³é”®è¯

3. **Semantic Scholar**
   - é“¾æ¥ï¼šhttps://www.semanticscholar.org/
   - ç‰¹è‰²ï¼šè®ºæ–‡å…³è”å›¾è°±ã€å½±å“åŠ›è¯„åˆ†

4. **Papers with Code**
   - é“¾æ¥ï¼šhttps://paperswithcode.com/
   - ç‰¹è‰²ï¼šè®ºæ–‡+ä»£ç +æ¦œå•ï¼Œä¾¿äºå¤ç°

5. **Connected Papers**
   - é“¾æ¥ï¼šhttps://www.connectedpapers.com/
   - ç‰¹è‰²ï¼šå¯è§†åŒ–è®ºæ–‡å…³è”ç½‘ç»œ

### é¡¶çº§ä¼šè®®æœŸåˆŠ

#### AI/MLé¡¶ä¼šï¼ˆæŸ¥æ‰¾æœ€æ–°ç ”ç©¶ï¼‰

- **NeurIPS** (Neural Information Processing Systems)
  - ç½‘ç«™ï¼šhttps://nips.cc/
  - å…³æ³¨ï¼šæ¨¡å‹æ¶æ„ã€ä¼˜åŒ–ç®—æ³•

- **ICML** (International Conference on Machine Learning)
  - ç½‘ç«™ï¼šhttps://icml.cc/

- **ICLR** (International Conference on Learning Representations)
  - ç½‘ç«™ï¼šhttps://iclr.cc/

#### ç³»ç»Ÿé¡¶ä¼šï¼ˆæŸ¥æ‰¾ç³»ç»Ÿä¼˜åŒ–ï¼‰

- **OSDI** (Operating Systems Design and Implementation)
  - å…³æ³¨ï¼šMLç³»ç»Ÿã€åˆ†å¸ƒå¼æ¨ç†

- **SOSP** (Symposium on Operating Systems Principles)

- **MLSys** (Conference on Machine Learning and Systems)
  - ç½‘ç«™ï¼šhttps://mlsys.org/
  - é‡è¦æ€§ï¼šâ­â­â­â­â­ (MLç³»ç»Ÿä¸“å±ä¼šè®®)

- **ASPLOS** (Architectural Support for Programming Languages and OS)
  - å…³æ³¨ï¼šç¡¬ä»¶-è½¯ä»¶ååŒè®¾è®¡

#### èŠ¯ç‰‡æ¶æ„é¡¶ä¼š

- **ISCA** (International Symposium on Computer Architecture)
- **MICRO** (IEEE/ACM International Symposium on Microarchitecture)
- **HPCA** (High-Performance Computer Architecture)

---

## ğŸ¢ é¡¶çº§å…¬å¸æŠ€æœ¯åšå®¢

### å¿…è¯»æŠ€æœ¯åšå®¢

1. **NVIDIA Technical Blog**
   - é“¾æ¥ï¼šhttps://developer.nvidia.com/blog/
   - å…³æ³¨ï¼šCUDAä¼˜åŒ–ã€TensorRT

2. **Google AI Blog**
   - é“¾æ¥ï¼šhttps://ai.googleblog.com/

3. **Meta AI Blog**
   - é“¾æ¥ï¼šhttps://ai.meta.com/blog/

4. **OpenAI Blog**
   - é“¾æ¥ï¼šhttps://openai.com/blog/

5. **Hugging Face Blog**
   - é“¾æ¥ï¼šhttps://huggingface.co/blog
   - é‡ç‚¹ï¼šæ¨¡å‹ä¼˜åŒ–å®è·µæ•™ç¨‹

6. **Microsoft Research Blog**
   - é“¾æ¥ï¼šhttps://www.microsoft.com/en-us/research/blog/

### ä¸­æ–‡æŠ€æœ¯ç¤¾åŒº

- **æœºå™¨ä¹‹å¿ƒ**ï¼šhttps://www.jiqizhixin.com/
- **AIç§‘æŠ€è¯„è®º**ï¼šhttps://www.leiphone.com/category/ai
- **PaperWeekly**ï¼šhttps://www.paperweekly.site/

---

## ğŸ“Š æŠ€æœ¯è·¯å¾„å…³é”®è®ºæ–‡å›¾è°±

```infographic
infographic hierarchy-mindmap-curved-line-compact-card
data
  title AIèŠ¯ç‰‡åº”ç”¨å¼€å‘æŠ€æœ¯è·¯å¾„
  items
    - label TransformeråŸºç¡€
      children
        - label Attention Is All You Need
        - label BERT
        - label GPT-3
    - label é«˜æ•ˆæ¨ç†
      children
        - label Flash Attention 1&2
          desc å¿…è¯»â­â­â­â­â­
        - label vLLM PagedAttention
          desc å¿…è¯»â­â­â­â­â­
        - label Speculative Decoding
    - label æ¨¡å‹å‹ç¼©
      children
        - label GPTQé‡åŒ–
          desc å¿…è¯»â­â­â­â­â­
        - label AWQé‡åŒ–
          desc MIT Han Lab
        - label LLM.int8()
    - label èŠ¯ç‰‡æ¶æ„
      children
        - label Google TPUè®ºæ–‡
          desc å¿…è¯»â­â­â­â­â­
        - label NVIDIA GPUæ¶æ„
        - label CUDAç¼–ç¨‹æŒ‡å—
    - label ç³»ç»Ÿä¼˜åŒ–
      children
        - label TVMç¼–è¯‘å™¨
        - label DeepSpeed
        - label Rayåˆ†å¸ƒå¼
```

---

## ğŸ¯ æŒ‰è§’è‰²æ¨èé˜…è¯»è·¯å¾„

### è·¯å¾„1ï¼šå…¥é—¨è€…ï¼ˆå‰4å‘¨ï¼‰
1. Attention Is All You Need
2. Flash Attention 1
3. LLM.int8()
4. vLLMè®ºæ–‡
5. Google TPUè®ºæ–‡

### è·¯å¾„2ï¼šè¿›é˜¶è€…ï¼ˆ5-8å‘¨ï¼‰
6. Flash Attention 2
7. GPTQ/AWQé‡åŒ–è®ºæ–‡
8. Speculative Decoding
9. TVMç¼–è¯‘å™¨è®ºæ–‡
10. LLaMA 1&2æŠ€æœ¯æŠ¥å‘Š

### è·¯å¾„3ï¼šä¸“å®¶çº§ï¼ˆ9-12å‘¨ï¼‰
- é˜…è¯»MLSys/OSDIæœ€æ–°è®ºæ–‡
- æ·±å…¥CUDAç¼–ç¨‹æŒ‡å—
- ç ”ç©¶ç‰¹å®šèŠ¯ç‰‡ç™½çš®ä¹¦
- è·Ÿè¸ªæ›¦æœ›sunriseçš„æŠ€æœ¯åšå®¢

---

## ğŸ“… è®ºæ–‡é˜…è¯»å»ºè®®

### é˜…è¯»æ–¹æ³•
1. **ä¸‰éé˜…è¯»æ³•**
   - ç¬¬ä¸€éï¼šè¯»æ‘˜è¦ã€ç»“è®ºã€å›¾è¡¨ï¼ˆ10åˆ†é’Ÿï¼‰
   - ç¬¬äºŒéï¼šè¯»å¼•è¨€ã€æ–¹æ³•ã€å®éªŒï¼ˆ30åˆ†é’Ÿï¼‰
   - ç¬¬ä¸‰éï¼šç²¾è¯»å¹¶å¤ç°å…³é”®ä»£ç ï¼ˆ2-3å°æ—¶ï¼‰

2. **åšç¬”è®°**
   - ä½¿ç”¨Notion/Obsidianå»ºç«‹è®ºæ–‡åº“
   - è®°å½•ï¼šé—®é¢˜ã€æ–¹æ³•ã€åˆ›æ–°ç‚¹ã€ä»£ç é“¾æ¥

3. **å¤ç°å®éªŒ**
   - ä¼˜å…ˆé€‰æ‹©æœ‰å¼€æºä»£ç çš„è®ºæ–‡
   - åœ¨å°æ•°æ®é›†ä¸ŠéªŒè¯

### æ—¶é—´å®‰æ’
- **æ¯å‘¨ç›®æ ‡**ï¼šç²¾è¯»1ç¯‡æ ¸å¿ƒè®ºæ–‡ + æ³›è¯»3-5ç¯‡ç›¸å…³è®ºæ–‡
- **å»ºç«‹ä¹ æƒ¯**ï¼šæ¯å¤©æ—©ä¸Š30åˆ†é’Ÿæµè§ˆarXivæ–°è®ºæ–‡

---

## ğŸ”— å¿«é€Ÿè®¿é—®æ¸…å•

### æ¯æ—¥å¿…è®¿
- [ ] arXiv cs.LG/cs.AI æ–°è®ºæ–‡
- [ ] Hugging Face Papersæ¯æ—¥æ¨è
- [ ] Papers with Code Trending

### æ¯å‘¨å¿…è®¿
- [ ] NVIDIA/Google AIåšå®¢æ›´æ–°
- [ ] æ›¦æœ›sunriseæŠ€æœ¯åŠ¨æ€
- [ ] é¡¶ä¼šaccepted papersåˆ—è¡¨

### å»ºç«‹RSSè®¢é˜…
æ¨èå·¥å…·ï¼šFeedly, Inoreader
è®¢é˜…æºï¼š
- arXiv RSS
- å„å¤§å…¬å¸æŠ€æœ¯åšå®¢
- é¡¶ä¼šé€šçŸ¥

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**æœ€åæ›´æ–°**ï¼š2026-01-07  
**ä¸‹æ¬¡æ›´æ–°**ï¼šè·Ÿè¸ª2026å¹´æœ€æ–°ä¼šè®®è®ºæ–‡
