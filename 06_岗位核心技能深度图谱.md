# æ ¸å¿ƒæŠ€èƒ½æ·±åº¦å›¾è°±


```infographic
infographic hierarchy-tree-curved-line-rounded-rect-node
data
  title Tå­—å‹æŠ€èƒ½ä½“ç³»
  items
    - label å¹¿åº¦ï¼šå…¨æµç¨‹è¦†ç›–
      children:
        - label åº•å±‚ç¡¬ä»¶
        - label æ¨¡å‹éƒ¨ç½²
        - label åº”ç”¨å¼€å‘
        - label æµ‹è¯•æ–‡æ¡£
    - label æ·±åº¦ï¼šæ ¸å¿ƒèšç„¦
      children:
        - label æ¨¡å‹éƒ¨ç½²ä¼˜åŒ–
        - label åç«¯å¼€å‘
        - label èŠ¯ç‰‡SDKé›†æˆ
```

---

## ğŸ¯ æŠ€èƒ½ä¼˜å…ˆçº§çŸ©é˜µ

```infographic
infographic quadrant-quarter-simple-card
data
  title æŠ€èƒ½å­¦ä¹ ä¼˜å…ˆçº§
  items
    - label ç¬¬ä¸€ä¼˜å…ˆçº§
      desc Python + Linux + æ¨¡å‹éƒ¨ç½²
      quadrant 1
    - label ç¬¬äºŒä¼˜å…ˆçº§
      desc é£ä¹¦å¼€å‘ + RAGæŠ€æœ¯
      quadrant 2
    - label ç¬¬ä¸‰ä¼˜å…ˆçº§
      desc Docker + é‡åŒ–æŠ€æœ¯
      quadrant 3
    - label ç¬¬å››ä¼˜å…ˆçº§
      desc C++ + èŠ¯ç‰‡è°ƒè¯•
      quadrant 4
```

---

## ä¸€ã€AIæ¨¡å‹ä¸éƒ¨ç½² (æ ¸å¿ƒä¸­çš„æ ¸å¿ƒ)

### 1.1 å¤§è¯­è¨€æ¨¡å‹(LLM)åŸºç¡€

#### ğŸ“š ç†è®ºçŸ¥è¯†

**Transformeræ¶æ„æ·±åº¦ç†è§£**

```python
# å…³é”®æ¦‚å¿µåœ°å›¾
æ ¸å¿ƒç»„ä»¶:
â”œâ”€â”€ Self-Attentionæœºåˆ¶
â”‚   â”œâ”€â”€ Q (Query)ã€K (Key)ã€V (Value) çŸ©é˜µ
â”‚   â”œâ”€â”€ æ³¨æ„åŠ›è®¡ç®—: Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V
â”‚   â””â”€â”€ Multi-Head Attention (å¤šå¤´æ³¨æ„åŠ›)
â”‚
â”œâ”€â”€ Position Encoding (ä½ç½®ç¼–ç )
â”‚   â”œâ”€â”€ ä¸ºä»€ä¹ˆéœ€è¦: Transformeræ²¡æœ‰å¾ªç¯ç»“æ„,ä¸çŸ¥é“tokené¡ºåº
â”‚   â””â”€â”€ ä¸¤ç§æ–¹å¼: ç»å¯¹ä½ç½®ç¼–ç  vs ç›¸å¯¹ä½ç½®ç¼–ç (RoPE)
â”‚
â”œâ”€â”€ Feed-Forward Network
â”‚   â””â”€â”€ MLP: Linear -> GELU/SiLU -> Linear
â”‚
â””â”€â”€ Layer Normalization
    â””â”€â”€ ä¸ºä»€ä¹ˆç”¨LayerNormè€Œä¸æ˜¯BatchNorm
```

**å¿…é¡»ç†è§£çš„æ ¸å¿ƒæ¦‚å¿µ**

1. **Tokenä¸Tokenization**
```python
# å®è·µï¼šç†è§£åˆ†è¯è¿‡ç¨‹
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)

text = "AIèŠ¯ç‰‡åº”ç”¨å¼€å‘"
tokens = tokenizer.encode(text)
print(f"åŸå§‹æ–‡æœ¬: {text}")
print(f"Token IDs: {tokens}")
print(f"Tokenæ•°é‡: {len(tokens)}")

# è§£ç å›æ–‡æœ¬
decoded = tokenizer.decode(tokens)
print(f"è§£ç å: {decoded}")

# å…³é”®ç†è§£ï¼š
# - ä¸ºä»€ä¹ˆä¸­æ–‡é€šå¸¸æ¯”è‹±æ–‡æ¶ˆè€—æ›´å¤štoken?
# - Tokenæ•°é‡å¦‚ä½•å½±å“æ¨¡å‹æˆæœ¬?
```

2. **Context Window (ä¸Šä¸‹æ–‡çª—å£)**
```
ä»€ä¹ˆæ˜¯Context Window:
- æ¨¡å‹ä¸€æ¬¡èƒ½å¤„ç†çš„æœ€å¤§tokenæ•°é‡
- ç¤ºä¾‹: ChatGLM3-6B = 8K, GPT-4 = 128K

ä¸ºä»€ä¹ˆé‡è¦:
- è¶…è¿‡çª—å£é•¿åº¦çš„å¯¹è¯éœ€è¦æˆªæ–­æˆ–æ»‘åŠ¨çª—å£
- å½±å“RAGåº”ç”¨ä¸­èƒ½æ”¾å…¥å¤šå°‘èƒŒæ™¯çŸ¥è¯†

å®é™…å½±å“:
è¾“å…¥prompt (1000 tokens) + å¯¹è¯å†å² (2000 tokens) + æ£€ç´¢å†…å®¹ (3000 tokens) 
= 6000 tokens < 8K âœ… å¯ä»¥å¤„ç†
```

3. **KV Cache (é”®å€¼ç¼“å­˜)** â­â­â­â­â­
```python
# ä¸ºä»€ä¹ˆéœ€è¦KV Cache?

# åœºæ™¯ï¼šç”Ÿæˆ100ä¸ªtokençš„æ–‡æœ¬
# ä¸ä½¿ç”¨KV Cache:
#   ç”Ÿæˆç¬¬1ä¸ªè¯: è®¡ç®—1æ¬¡attention
#   ç”Ÿæˆç¬¬2ä¸ªè¯: é‡æ–°è®¡ç®—å‰2ä¸ªè¯çš„attention (æµªè´¹!)
#   ç”Ÿæˆç¬¬3ä¸ªè¯: é‡æ–°è®¡ç®—å‰3ä¸ªè¯çš„attention (æ›´æµªè´¹!)
#   ...
#   æ€»è®¡ç®—é‡: O(nÂ²)

# ä½¿ç”¨KV Cache:
#   ç”Ÿæˆç¬¬1ä¸ªè¯: è®¡ç®—å¹¶ç¼“å­˜K,VçŸ©é˜µ
#   ç”Ÿæˆç¬¬2ä¸ªè¯: å¤ç”¨ä¹‹å‰çš„K,Vï¼Œåªè®¡ç®—æ–°token
#   ...
#   æ€»è®¡ç®—é‡: O(n)

# ä»£ä»·ï¼šæ˜¾å­˜å ç”¨å¢åŠ 
# - FP16ç²¾åº¦: æ¯ä¸ªtokençš„KV Cache â‰ˆ 2 * hidden_size * num_layers * 2 bytes
# - ä»¥ChatGLM3-6Bä¸ºä¾‹ (hidden=4096, layers=28):
#   1000 tokensçš„KV Cache â‰ˆ 2 * 4096 * 28 * 2 * 1000 / 1024^3 â‰ˆ 0.43 GB
```

**åŠ¨æ‰‹å®è·µ**
```python
# å®éªŒ1: å¯¹æ¯”æœ‰æ— KV Cacheçš„æ¨ç†é€Ÿåº¦
import time
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).half().cuda()
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)

prompt = "è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"

# ä¸ä½¿ç”¨KV Cache
start = time.time()
response, history = model.chat(tokenizer, prompt, history=[], use_cache=False)
no_cache_time = time.time() - start

# ä½¿ç”¨KV Cache
start = time.time()
response, history = model.chat(tokenizer, prompt, history=[], use_cache=True)
cache_time = time.time() - start

print(f"ä¸ä½¿ç”¨KV Cache: {no_cache_time:.2f}s")
print(f"ä½¿ç”¨KV Cache: {cache_time:.2f}s")
print(f"åŠ é€Ÿæ¯”: {no_cache_time/cache_time:.2f}x")
```

---

**ä¸»æµæ¨¡å‹æ¶æ„å¯¹æ¯”**

| æ¨¡å‹ | å‚æ•°é‡ | Context Length | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|--------|----------------|------|----------|
| **Llama 2** | 7B-70B | 4K | å¼€æºã€ç¤¾åŒºæ´»è·ƒ | é€šç”¨åŸºåº§æ¨¡å‹ |
| **Qwen (é€šä¹‰åƒé—®)** | 1.8B-72B | 32K | é•¿æ–‡æœ¬ã€ä¸­æ–‡ä¼˜åŒ– | ä¼ä¸šåº”ç”¨ |
| **ChatGLM3** | 6B | 8K | å›½äº§ã€å¯¹è¯ä¼˜åŒ– | ä¸­æ–‡å¯¹è¯ |
| **DeepSeek** | 7B-67B | 4K-16K | ä»£ç èƒ½åŠ›å¼º | ç¼–ç¨‹åŠ©æ‰‹ |
| **Yi (é›¶ä¸€ä¸‡ç‰©)** | 6B-34B | 200K | è¶…é•¿ä¸Šä¸‹æ–‡ | é•¿æ–‡æ¡£åˆ†æ |

**ç›®å½•ç»“æ„ç†è§£**
```bash
# å…¸å‹çš„HuggingFaceæ¨¡å‹ç›®å½•
chatglm3-6b/
â”œâ”€â”€ config.json              # æ¨¡å‹é…ç½® (å±‚æ•°ã€hidden_sizeç­‰)
â”œâ”€â”€ generation_config.json   # ç”Ÿæˆå‚æ•°é»˜è®¤å€¼
â”œâ”€â”€ tokenizer_config.json    # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ tokenizer.model          # SentencePieceåˆ†è¯æ¨¡å‹
â”œâ”€â”€ pytorch_model.bin        # æ¨¡å‹æƒé‡ (æˆ–.safetensors)
â””â”€â”€ modeling_chatglm.py      # è‡ªå®šä¹‰æ¨¡å‹ä»£ç  (trust_remote_code=Trueæ—¶åŠ è½½)

# åŠ¨æ‰‹ï¼šæŸ¥çœ‹config.json
import json
with open("chatglm3-6b/config.json") as f:
    config = json.load(f)
    print(f"éšè—å±‚å¤§å°: {config['hidden_size']}")
    print(f"å±‚æ•°: {config['num_layers']}")
    print(f"æ³¨æ„åŠ›å¤´æ•°: {config['num_attention_heads']}")
```

---

#### ğŸ›ï¸ æ¨ç†å‚æ•°æ·±åº¦ç†è§£

**Temperature (æ¸©åº¦å‚æ•°)**

```python
# åŸç†ï¼šæ§åˆ¶softmaxçš„"å°–é”åº¦"
# logits: æ¨¡å‹è¾“å‡ºçš„æœªå½’ä¸€åŒ–åˆ†æ•°
# temperature = 1.0: æ ‡å‡†softmax
# temperature < 1.0: åˆ†å¸ƒæ›´å°–é”ï¼Œæ›´ç¡®å®šæ€§
# temperature > 1.0: åˆ†å¸ƒæ›´å¹³æ»‘ï¼Œæ›´éšæœº

import numpy as np
import matplotlib.pyplot as plt

logits = np.array([3.0, 2.0, 1.0, 0.5])  # æ¨¡æ‹Ÿ4ä¸ªå€™é€‰tokençš„åˆ†æ•°

def softmax_with_temp(logits, temperature):
    exp_logits = np.exp(logits / temperature)
    return exp_logits / exp_logits.sum()

# å¯¹æ¯”ä¸åŒæ¸©åº¦
temps = [0.1, 0.5, 1.0, 2.0]
for temp in temps:
    probs = softmax_with_temp(logits, temp)
    print(f"Temperature={temp}: {probs}")
    # Temperature=0.1: [0.876, 0.107, 0.013, 0.004]  # å‡ ä¹ç¡®å®šé€‰ç¬¬ä¸€ä¸ª
    # Temperature=2.0: [0.389, 0.287, 0.211, 0.113]  # æ›´å¹³å‡

# å®é™…åº”ç”¨:
# - å†™ä»£ç ã€ç¿»è¯‘: temperature=0.1-0.3 (è¦ç²¾å‡†)
# - åˆ›æ„å†™ä½œã€å¤´è„‘é£æš´: temperature=0.7-1.5 (è¦å¤šæ ·æ€§)
```

**Top-P (nucleus sampling) å’Œ Top-K**

```python
# Top-K: åªä»æ¦‚ç‡æœ€é«˜çš„Kä¸ªtokenä¸­é‡‡æ ·
# Top-P: ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°Pæ—¶åœæ­¢ï¼Œä»è¿™äº›tokenä¸­é‡‡æ ·

def top_k_top_p_filtering(logits, top_k=0, top_p=1.0):
    """
    logits: shape (vocab_size,)
    """
    # Top-Kè¿‡æ»¤
    if top_k > 0:
        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
        logits[indices_to_remove] = -float('Inf')
    
    # Top-Pè¿‡æ»¤
    if top_p < 1.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„token
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        logits[indices_to_remove] = -float('Inf')
    
    return logits

# å®é™…æ•ˆæœ:
# Top-K=50: åªè€ƒè™‘å‰50ä¸ªæœ€å¯èƒ½çš„è¯
# Top-P=0.9: è€ƒè™‘ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°90%çš„æœ€å°è¯é›†åˆ
# 
# Top-Pæ›´çµæ´»: 
# - ç¡®å®šæ€§é«˜çš„ä½ç½®(å¦‚"åŒ—äº¬æ˜¯ä¸­å›½çš„___"): å¯èƒ½åªéœ€è¦1-2ä¸ªè¯å°±è¾¾åˆ°90%
# - ä¸ç¡®å®šçš„ä½ç½®(å¦‚åˆ›æ„ç»­å†™): å¯èƒ½éœ€è¦å‡ åä¸ªè¯
```

**Repetition Penalty (é‡å¤æƒ©ç½š)**

```python
# é—®é¢˜: æ¨¡å‹å®¹æ˜“é™·å…¥é‡å¤å¾ªç¯
# ä¾‹å¦‚: "æˆ‘å¾ˆé«˜å…´,æˆ‘å¾ˆé«˜å…´,æˆ‘å¾ˆé«˜å…´..."

# åŸç†: å¯¹å·²ç”Ÿæˆçš„tokençš„logitsæ–½åŠ æƒ©ç½š
# repetition_penalty = 1.0: ä¸æƒ©ç½š
# repetition_penalty > 1.0: æƒ©ç½šé‡å¤ (å¸¸ç”¨1.1-1.3)

# ä¼ªä»£ç 
for token_id in generated_tokens:
    if logits[token_id] > 0:
        logits[token_id] /= repetition_penalty
    else:
        logits[token_id] *= repetition_penalty

# å®é™…åº”ç”¨:
# - å¯¹è¯ç³»ç»Ÿ: 1.1-1.2 (é¿å…è½¦è½±è¾˜è¯)
# - ä»£ç ç”Ÿæˆ: 1.0 (å…è®¸é‡å¤å…³é”®å­—å¦‚'for', 'if')
```

**åŠ¨æ‰‹å®è·µï¼šå‚æ•°è°ƒä¼˜å®éªŒ**

```python
# å®éªŒ: ç”¨ä¸åŒå‚æ•°ç”Ÿæˆæ–‡æœ¬,è§‚å¯Ÿæ•ˆæœ
model, tokenizer = ...  # åŠ è½½æ¨¡å‹

prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿æ˜¯"

configs = [
    {"temperature": 0.1, "top_p": 0.9, "repetition_penalty": 1.0},
    {"temperature": 1.0, "top_p": 0.9, "repetition_penalty": 1.0},
    {"temperature": 0.7, "top_p": 0.95, "repetition_penalty": 1.2},
]

for i, config in enumerate(configs):
    response = model.generate(
        tokenizer.encode(prompt),
        max_length=200,
        **config
    )
    print(f"\n=== é…ç½®{i+1} ===")
    print(config)
    print(tokenizer.decode(response[0]))
```

---

### 1.2 æ¨¡å‹éƒ¨ç½²ä¸ä¼˜åŒ–æŠ€æœ¯ â­â­â­â­â­

#### ğŸ”¢ é‡åŒ–æŠ€æœ¯æ·±åº¦è§£æ

**æ•°å€¼ç²¾åº¦åŸºç¡€**

```
æµ®ç‚¹æ•°è¡¨ç¤º (IEEE 754):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¦å·ä½  â”‚  æŒ‡æ•°ä½  â”‚  å°¾æ•°ä½   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FP32 (å•ç²¾åº¦): 1 + 8 + 23 = 32 bits
FP16 (åŠç²¾åº¦): 1 + 5 + 10 = 16 bits
BF16 (Brain Float): 1 + 8 + 7 = 16 bits
INT8 (æ•´æ•°): 8 bits

è¡¨ç¤ºèŒƒå›´å¯¹æ¯”:
- FP32: Â±3.4Ã—10Â³â¸
- FP16: Â±65,504
- BF16: Â±3.4Ã—10Â³â¸ (æŒ‡æ•°ä½ä¸FP32ç›¸åŒ,ä¸æ˜“æº¢å‡º)
- INT8: -128 ~ 127
```

**ä¸ºä»€ä¹ˆè¦é‡åŒ–?**

```python
# ä»¥ChatGLM-6Bä¸ºä¾‹
å‚æ•°é‡: 6B (60äº¿)

å­˜å‚¨ç©ºé—´:
- FP32: 6B Ã— 4 bytes = 24 GB
- FP16: 6B Ã— 2 bytes = 12 GB
- INT8: 6B Ã— 1 byte  = 6 GB
- INT4: 6B Ã— 0.5 byte = 3 GB

è‡ªç ”èŠ¯ç‰‡æ˜¾å­˜å‡è®¾: 8 GB
ç»“è®º: å¿…é¡»ç”¨INT8æˆ–æ›´ä½ç²¾åº¦æ‰èƒ½è£…è½½!
```

**é‡åŒ–æ–¹æ³•åˆ†ç±»**

```infographic
infographic hierarchy-tree-curved-line-rounded-rect-node
data
  title é‡åŒ–æŠ€æœ¯åˆ†ç±»
  items
    - label æŒ‰æ—¶æœºåˆ†ç±»
      children:
        - label QAT é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
          desc è®­ç»ƒæ—¶å°±æ¨¡æ‹Ÿé‡åŒ–
        - label PTQ è®­ç»ƒåé‡åŒ–
          desc ç›´æ¥è½¬æ¢å·²è®­ç»ƒæ¨¡å‹
    - label æŒ‰å¯¹è±¡åˆ†ç±»
      children:
        - label æƒé‡é‡åŒ–
          desc åªé‡åŒ–W
        - label æ¿€æ´»é‡åŒ–
          desc é‡åŒ–Wå’ŒA
    - label æŒ‰ç²¾åº¦åˆ†ç±»
      children:
        - label INT8
        - label INT4
        - label æ··åˆç²¾åº¦
```

**PTQ (è®­ç»ƒåé‡åŒ–) è¯¦è§£**

```python
# åŸç†: å°†FP32æƒé‡æ˜ å°„åˆ°INT8èŒƒå›´

# æ–¹æ³•1: å¯¹ç§°é‡åŒ–
# [-max, max] -> [-127, 127]
scale = max(abs(weights)) / 127
quantized_weights = round(weights / scale)
dequantized_weights = quantized_weights * scale

# æ–¹æ³•2: éå¯¹ç§°é‡åŒ–
# [min, max] -> [0, 255]
scale = (max(weights) - min(weights)) / 255
zero_point = round(-min(weights) / scale)
quantized_weights = round(weights / scale) + zero_point

# å®æˆ˜ä»£ç 
import torch

def quantize_tensor(tensor, num_bits=8):
    """å¯¹ç§°é‡åŒ–"""
    max_val = tensor.abs().max()
    scale = max_val / (2 ** (num_bits - 1) - 1)
    
    quantized = torch.round(tensor / scale)
    quantized = torch.clamp(quantized, -(2**(num_bits-1)), 2**(num_bits-1)-1)
    
    # æ¨¡æ‹Ÿåé‡åŒ–
    dequantized = quantized * scale
    
    # è®¡ç®—è¯¯å·®
    error = (tensor - dequantized).abs().mean()
    
    return quantized.to(torch.int8), scale, error

# æµ‹è¯•
weight = torch.randn(1000, 1000)  # æ¨¡æ‹Ÿæƒé‡çŸ©é˜µ
quantized, scale, error = quantize_tensor(weight)

print(f"åŸå§‹æƒé‡èŒƒå›´: [{weight.min():.4f}, {weight.max():.4f}]")
print(f"é‡åŒ–scale: {scale:.6f}")
print(f"å¹³å‡è¯¯å·®: {error:.6f}")
```

**ä¸»æµé‡åŒ–åº“å¯¹æ¯”**

| æ–¹æ³• | ç²¾åº¦ | é€Ÿåº¦ | æ˜¾å­˜ | é€‚ç”¨åœºæ™¯ | éš¾åº¦ |
|------|------|------|------|----------|------|
| **bitsandbytes (LLM.int8())** | INT8 | ä¸­ | â†“46% | é€šç”¨ | â­ ç®€å• |
| **GPTQ** | INT4/INT8 | å¿« | â†“75% | ç¦»çº¿éƒ¨ç½² | â­â­ ä¸­ç­‰ |
| **AWQ** | INT4 | æœ€å¿« | â†“75% | æ¨ç†ä¼˜åŒ– | â­â­â­ è¾ƒéš¾ |
| **SmoothQuant** | INT8 | å¿« | â†“50% | æ¿€æ´»é‡åŒ– | â­â­â­ è¾ƒéš¾ |

**åŠ¨æ‰‹å®è·µ1: bitsandbytesé‡åŒ–**

```python
# æœ€ç®€å•çš„é‡åŒ–æ–¹æ³•
from transformers import AutoModel, BitsAndBytesConfig
import torch

# é…ç½®INT8é‡åŒ–
quant_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,  # æ··åˆç²¾åº¦é˜ˆå€¼
    llm_int8_has_fp16_weight=False
)

# åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModel.from_pretrained(
    "THUDM/chatglm3-6b",
    quantization_config=quant_config,
    device_map="auto",
    trust_remote_code=True
)

# æŸ¥çœ‹æ˜¾å­˜å ç”¨
print(f"æ¨¡å‹æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

# æ¨ç†æµ‹è¯•
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)

response, _ = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
```

**åŠ¨æ‰‹å®è·µ2: GPTQé‡åŒ–**

```python
# GPTQ: æ›´æ¿€è¿›çš„é‡åŒ–,æ”¯æŒINT4
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# å‡†å¤‡æ ¡å‡†æ•°æ® (ç”¨äºè®¡ç®—é‡åŒ–å‚æ•°)
calibration_data = [
    "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†",
    # ... æ›´å¤šä»£è¡¨æ€§æ–‡æœ¬
]

# é‡åŒ–é…ç½®
quantize_config = BaseQuantizeConfig(
    bits=4,  # INT4é‡åŒ–
    group_size=128,  # åˆ†ç»„é‡åŒ–
    desc_act=False
)

# æ‰§è¡Œé‡åŒ–
model = AutoGPTQForCausalLM.from_pretrained(
    "THUDM/chatglm3-6b",
    quantize_config=quantize_config
)

model.quantize(calibration_data)

# ä¿å­˜é‡åŒ–æ¨¡å‹
model.save_quantized("chatglm3-6b-gptq-int4")

# åŠ è½½ä½¿ç”¨
model = AutoGPTQForCausalLM.from_quantized("chatglm3-6b-gptq-int4")
```

**é‡åŒ–æ•ˆæœå¯¹æ¯”å®éªŒ**

```python
# å®éªŒè®¾è®¡: å¯¹æ¯”ä¸åŒé‡åŒ–æ–¹æ³•çš„æ€§èƒ½

import time
import torch
from transformers import AutoModel, AutoTokenizer

test_prompts = [
    "è§£é‡Šé‡å­è®¡ç®—çš„åŸç†",
    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
    "ç”¨Pythonå®ç°å¿«é€Ÿæ’åº"
]

def benchmark_model(model, tokenizer, prompts):
    results = {
        "latency": [],
        "memory_gb": 0,
        "responses": []
    }
    
    for prompt in prompts:
        start = time.time()
        response, _ = model.chat(tokenizer, prompt, history=[])
        latency = time.time() - start
        
        results["latency"].append(latency)
        results["responses"].append(response)
    
    results["memory_gb"] = torch.cuda.max_memory_allocated() / 1024**3
    results["avg_latency"] = sum(results["latency"]) / len(results["latency"])
    
    return results

# æµ‹è¯•ä¸åŒé…ç½®
configs = {
    "FP16": {"torch_dtype": torch.float16},
    "INT8": {"load_in_8bit": True},
    "INT4-GPTQ": {"model_path": "chatglm3-6b-gptq-int4"}
}

# è¿è¡Œå¯¹æ¯”...
```

---

#### ğŸ”„ æ¨¡å‹æ ¼å¼è½¬æ¢

**ONNXåè®®è¯¦è§£**

```
ä»€ä¹ˆæ˜¯ONNX?
- Open Neural Network Exchange (å¼€æ”¾ç¥ç»ç½‘ç»œäº¤æ¢)
- ä¸€ç§ä¸­é—´è¡¨ç¤ºæ ¼å¼,ç±»ä¼¼ç¼–ç¨‹è¯­è¨€çš„å­—èŠ‚ç 
- æ”¯æŒè·¨æ¡†æ¶: PyTorch -> ONNX -> TensorRT/OpenVINO/è‡ªç ”æ¨ç†å¼•æ“

ä¸ºä»€ä¹ˆéœ€è¦ONNX?
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PyTorch â”‚â”€â”€â”€â”€â†’â”‚ ONNX â”‚â”€â”€â”€â”€â†’â”‚ è‡ªç ”èŠ¯ç‰‡ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              (é€šç”¨æ ¼å¼)      (ä¸“ç”¨æ¨ç†)

ä¼˜åŠ¿:
âœ… æ¡†æ¶æ— å…³: è®­ç»ƒç”¨PyTorch,éƒ¨ç½²ç”¨TensorRT
âœ… ä¼˜åŒ–å›¾: ONNX Runtimeä¼šåšå›¾ä¼˜åŒ–
âœ… ç¡¬ä»¶é€‚é…: èŠ¯ç‰‡å‚å•†åªéœ€æ”¯æŒONNX
```

**åŠ¨æ‰‹å®è·µ: PyTorch -> ONNX**

```python
# ç¤ºä¾‹: å¯¼å‡ºä¸€ä¸ªç®€å•çš„Transformeræ¨¡å‹

import torch
import torch.nn as nn

class SimpleTransformer(nn.Module):
    def __init__(self, d_model=512, nhead=8):
        super().__init__()
        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)
        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=6)
    
    def forward(self, src):
        return self.transformer(src)

model = SimpleTransformer()
model.eval()

# å‡†å¤‡dummy input
dummy_input = torch.randn(10, 32, 512)  # (seq_len, batch, d_model)

# å¯¼å‡ºONNX
torch.onnx.export(
    model,
    dummy_input,
    "transformer.onnx",
    export_params=True,
    opset_version=14,  # ONNXç®—å­ç‰ˆæœ¬
    do_constant_folding=True,  # å¸¸é‡æŠ˜å ä¼˜åŒ–
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': {0: 'seq_len', 1: 'batch'},
        'output': {0: 'seq_len', 1: 'batch'}
    }  # æ”¯æŒåŠ¨æ€shape
)

print("å¯¼å‡ºæˆåŠŸ! æ–‡ä»¶: transformer.onnx")
```

**ä½¿ç”¨Netronå¯è§†åŒ–ONNX**

```bash
# å®‰è£…Netron
pip install netron

# å¯åŠ¨å¯è§†åŒ–
netron transformer.onnx

# åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹:
# - æ¨¡å‹ç»“æ„å›¾
# - æ¯å±‚çš„å‚æ•°shape
# - ç®—å­ç±»å‹
```

**ONNX Runtimeæ¨ç†**

```python
import onnxruntime as ort
import numpy as np

# åˆ›å»ºæ¨ç†session
session = ort.InferenceSession("transformer.onnx")

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
print("è¾“å…¥:")
for input in session.get_inputs():
    print(f"  {input.name}: {input.shape} ({input.type})")

print("è¾“å‡º:")
for output in session.get_outputs():
    print(f"  {output.name}: {output.shape} ({output.type})")

# æ¨ç†
input_data = np.random.randn(10, 32, 512).astype(np.float32)
outputs = session.run(None, {"input": input_data})

print(f"è¾“å‡ºshape: {outputs[0].shape}")
```

**å¸¸è§æ ¼å¼å¯¹æ¯”**

| æ ¼å¼ | æ¥æº | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **.pth / .pt** | PyTorch | Pythonåºåˆ—åŒ– | è®­ç»ƒcheckpoint |
| **.onnx** | ONNX | è·¨æ¡†æ¶æ ‡å‡† | æ¨¡å‹äº¤æ¢ |
| **.pb** | TensorFlow | Protocol Buffer | TFæ¨¡å‹ |
| **.trt** | TensorRT | NVIDIAä¼˜åŒ– | GPUæ¨ç† |
| **.om** | æ˜‡è…¾ | åä¸ºæ ¼å¼ | æ˜‡è…¾èŠ¯ç‰‡ |
| **è‡ªç ”æ ¼å¼** | æ›¦æœ›? | å¾…äº†è§£ | è‡ªç ”èŠ¯ç‰‡ |

---

#### ğŸ“Š æ€§èƒ½æŒ‡æ ‡æ·±åº¦ç†è§£

**TPS (Tokens Per Second)**

```python
# å®šä¹‰: æ¯ç§’ç”Ÿæˆçš„tokenæ•°é‡
# è®¡ç®—å…¬å¼:
TPS = ç”Ÿæˆçš„tokenæ•° / ç”Ÿæˆè€—æ—¶(ç§’)

# ç¤ºä¾‹
ç”Ÿæˆ100ä¸ªtoken,è€—æ—¶5ç§’
TPS = 100 / 5 = 20 tokens/s

# å½±å“å› ç´ :
1. æ¨¡å‹å¤§å° (å‚æ•°é‡è¶Šå¤§,è®¡ç®—è¶Šæ…¢)
2. batch size (æ‰¹å¤„ç†æå‡åå)
3. ç¡¬ä»¶ (GPU/èŠ¯ç‰‡ç®—åŠ›)
4. ä¼˜åŒ–æŠ€æœ¯ (é‡åŒ–ã€ç®—å­èåˆ)

# æµ‹é‡ä»£ç 
import time

def measure_tps(model, tokenizer, prompt, max_new_tokens=100):
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    start = time.time()
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False
    )
    duration = time.time() - start
    
    generated_tokens = outputs.shape[1] - inputs['input_ids'].shape[1]
    tps = generated_tokens / duration
    
    print(f"ç”Ÿæˆtokens: {generated_tokens}")
    print(f"è€—æ—¶: {duration:.2f}s")
    print(f"TPS: {tps:.2f} tokens/s")
    
    return tps
```

**Latency (å»¶è¿Ÿ)**

```python
# ä¸¤ç§å…³é”®å»¶è¿Ÿ:

1. TTFT (Time To First Token)
   - ä»è¾“å…¥åˆ°ç¬¬ä¸€ä¸ªtokenè¾“å‡ºçš„æ—¶é—´
   - ç”¨æˆ·ä½“éªŒå…³é”®æŒ‡æ ‡
   - ä¸»è¦ç”±prefillé˜¶æ®µå†³å®š

2. TPOT (Time Per Output Token)  
   - ç”Ÿæˆæ¯ä¸ªtokençš„å¹³å‡æ—¶é—´
   - decodeé˜¶æ®µçš„é€Ÿåº¦

# æµ‹é‡ä»£ç 
def measure_latency(model, tokenizer, prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    # æµ‹é‡TTFT
    start = time.time()
    # ç”Ÿæˆç¬¬ä¸€ä¸ªtoken (éœ€è¦æµå¼API)
    first_token_time = time.time() - start
    
    # ç”Ÿæˆå®Œæ•´å“åº”
    start_full = time.time()
    outputs = model.generate(**inputs, max_new_tokens=50)
    total_time = time.time() - start_full
    
    num_tokens = outputs.shape[1] - inputs['input_ids'].shape[1]
    tpot = (total_time - first_token_time) / (num_tokens - 1)
    
    print(f"TTFT: {first_token_time*1000:.0f}ms")
    print(f"TPOT: {tpot*1000:.0f}ms/token")
```

**Throughput (ååé‡)**

```python
# å®šä¹‰: å•ä½æ—¶é—´å¤„ç†çš„è¯·æ±‚æ•°
# ä¸TPSåŒºåˆ«:
# - TPS: å•ä¸ªè¯·æ±‚çš„ç”Ÿæˆé€Ÿåº¦
# - Throughput: ç³»ç»Ÿæ•´ä½“çš„å¤„ç†èƒ½åŠ›

# æµ‹é‡: å¹¶å‘è¯·æ±‚æµ‹è¯•
import asyncio
import aiohttp

async def send_request(session, prompt):
    async with session.post(
        "http://localhost:8000/generate",
        json={"prompt": prompt, "max_tokens": 100}
    ) as resp:
        return await resp.json()

async def benchmark_throughput(num_concurrent=10, num_requests=100):
    async with aiohttp.ClientSession() as session:
        tasks = []
        start = time.time()
        
        for i in range(num_requests):
            task = send_request(session, f"è¯·æ±‚{i}")
            tasks.append(task)
            
            # æ§åˆ¶å¹¶å‘æ•°
            if len(tasks) >= num_concurrent:
                await asyncio.gather(*tasks)
                tasks = []
        
        # å¤„ç†å‰©ä½™è¯·æ±‚
        if tasks:
            await asyncio.gather(*tasks)
        
        duration = time.time() - start
        throughput = num_requests / duration
        
        print(f"æ€»è¯·æ±‚æ•°: {num_requests}")
        print(f"å¹¶å‘æ•°: {num_concurrent}")
        print(f"æ€»è€—æ—¶: {duration:.2f}s")
        print(f"ååé‡: {throughput:.2f} req/s")

# è¿è¡Œ
asyncio.run(benchmark_throughput())
```

**æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•**

```markdown
# æ€§èƒ½ä¼˜åŒ–Checklist

## æ¨¡å‹å±‚é¢
- [ ] é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¤§å° (6B vs 13B vs 70B)
- [ ] åº”ç”¨é‡åŒ– (FP16 -> INT8 -> INT4)
- [ ] å¯ç”¨KV Cache
- [ ] Flash Attention (å¦‚æœæ”¯æŒ)

## ç³»ç»Ÿå±‚é¢  
- [ ] æ‰¹å¤„ç† (batch inference)
- [ ] ä½¿ç”¨vLLM/TensorRTç­‰æ¨ç†å¼•æ“
- [ ] GPU/èŠ¯ç‰‡å¹¶è¡Œ
- [ ] ç®—å­èåˆä¼˜åŒ–

## åº”ç”¨å±‚é¢
- [ ] å¼‚æ­¥å¤„ç†
- [ ] è¯·æ±‚é˜Ÿåˆ—ç®¡ç†
- [ ] ç¼“å­˜å¸¸è§æŸ¥è¯¢
- [ ] é™æµä¸é™çº§

## ç›‘æ§æŒ‡æ ‡
- [ ] TPS > 20 (ç”Ÿæˆé€Ÿåº¦)
- [ ] TTFT < 500ms (ç”¨æˆ·æ„ŸçŸ¥)
- [ ] GPUåˆ©ç”¨ç‡ > 70%
- [ ] æ˜¾å­˜å ç”¨ < 80%
```

---

## ğŸ“ å­¦ä¹ è·¯çº¿å›¾

```infographic
infographic sequence-snake-steps-simple
data
  title LLMéƒ¨ç½²æŠ€èƒ½å­¦ä¹ è·¯çº¿
  items
    - label ç¬¬1å‘¨ï¼šTransformeråŸºç¡€
      desc ç†è§£attentionã€position encoding
    - label ç¬¬2å‘¨ï¼šæ¨¡å‹åŠ è½½ä¸æ¨ç†
      desc HuggingFace Transformerså®æˆ˜
    - label ç¬¬3å‘¨ï¼šé‡åŒ–æŠ€æœ¯
      desc bitsandbytesã€GPTQå®è·µ
    - label ç¬¬4å‘¨ï¼šONNXè½¬æ¢
      desc æ¨¡å‹å¯¼å‡ºä¸ä¼˜åŒ–
    - label ç¬¬5å‘¨ï¼šæ€§èƒ½æµ‹è¯•
      desc TPSã€å»¶è¿Ÿã€ååé‡æµ‹é‡
    - label ç¬¬6å‘¨ï¼šç»¼åˆé¡¹ç›®
      desc éƒ¨ç½²ä¼˜åŒ–å®Œæ•´æµç¨‹
```

---

**æœ¬èŠ‚æ€»ç»“**

âœ… **ç†è®ºæŒæ¡**ï¼šTransformeræ¶æ„ã€KV Cacheã€æ¨ç†å‚æ•°
âœ… **æŠ€æœ¯æ·±å…¥**ï¼šé‡åŒ–åŸç†ã€ONNXè½¬æ¢ã€æ€§èƒ½æŒ‡æ ‡
âœ… **åŠ¨æ‰‹å®è·µ**ï¼šä»£ç ç¤ºä¾‹ã€æµ‹è¯•æ–¹æ³•ã€ä¼˜åŒ–æŠ€å·§

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼šç³»ç»Ÿä¸å·¥å…·é“¾ - Linuxã€Dockerã€èŠ¯ç‰‡SDKé›†æˆ

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¶é—´**: 2026-01-07  
